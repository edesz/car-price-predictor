{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scrape car listings](#scrape-car-listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import nan as np_nan\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Web Scraping and export to file](#web-scraping-and-export-to-file)\n",
    "3. [Compare saved and scraped data](#compare-saved-and-scraped-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use `bs4` to perform the web-scraping of listing details based on the listing IDs we retrieved in `2_cars_listings_controller.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll specify some input variables and helper functions to be used later in this notebook, two of which are discussed below\n",
    "- `page_number`<sup>[1](#myfootnote1)</sup>\n",
    "  - this is an integer starting at 0\n",
    "- `listing_number`<sup>[1](#myfootnote1)</sup>\n",
    "  - this is an integer starting at 0, with a maximum possible value of 99\n",
    "\n",
    "  <a name=\"myfootnote1\">1</a>: this does not have to be correct but is just for the loop in the 2nd last cell to run correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "page_number = 1\n",
    "listing_number = 64\n",
    "fpath = Path().cwd() / \"data\"\n",
    "html_file_path = fpath / \"p1a.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_clean_data(df, pcols=[\"msrp\", \"dealer_price\"]):\n",
    "    \"\"\"Remove dollar-sign and convert to float.\"\"\"\n",
    "    df[pcols] = df[pcols].replace(\"[\\$,]\", \"\", regex=True).astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rearrange_columns(\n",
    "    df,\n",
    "    pcols=[\"msrp\", \"dealer_price\"],\n",
    "    cols_to_move_to_front=[\"link\", \"listing_number\", \"page_number\"]\n",
    "):\n",
    "    \"\"\"Re-arrange columns and get minimum of msrp and dealer_price columns.\"\"\"\n",
    "    # Append target column as minimum of two price columns\n",
    "    df[\"price\"] = df[pcols].min(axis=1, skipna=True)\n",
    "\n",
    "    # Move some columns to front of df\n",
    "    cols = df.columns.tolist()\n",
    "    for c in cols_to_move_to_front:\n",
    "        cols.insert(0, cols.pop(cols.index(c)))\n",
    "    df = df.reindex(columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_listing(soup, page_number=1, listing_number=1):\n",
    "    \"\"\"Scrape a single listing.\"\"\"\n",
    "    d = {}\n",
    "    e = {}\n",
    "    for k in [\n",
    "        \"Fuel Type\",\n",
    "        \"City MPG\",\n",
    "        \"Highway MPG\",\n",
    "        \"Drivetrain\",\n",
    "        \"Engine\",\n",
    "        \"Mileage\",\n",
    "        \"Exterior Color\",\n",
    "        \"Interior Color\",\n",
    "        \"Stock\",\n",
    "        \"Transmission\",\n",
    "        \"VIN\",\n",
    "        \"seller_address\",\n",
    "        \"seller_zip\",\n",
    "        \"seller_rating\",\n",
    "        \"seller_reviews\",\n",
    "        \"type\",\n",
    "        \"title\",\n",
    "        \"miles\",\n",
    "        \"dealer_price\",\n",
    "        \"msrp\",\n",
    "        \"consumer_stars\",\n",
    "        \"consumer_reviews\",\n",
    "        \"Comfort\",\n",
    "        \"Performance\",\n",
    "        \"Exterior Styling\",\n",
    "        \"Interior Design\",\n",
    "        \"Value for the Money\",\n",
    "        \"Reliability\",\n",
    "    ]:\n",
    "        d[k] = np_nan\n",
    "\n",
    "    # Add page number, listing number, url to dictionary\n",
    "    d[\"page_number\"] = page_number\n",
    "    d[\"listing_number\"] = listing_number\n",
    "    try:\n",
    "        url_link = \"https://www.cars.com/vehicledetail/detail/\" + (\n",
    "            soup.find(\"section\", {\"class\": \"vehicle-info\"}).find(\"a\")[\"href\"]\n",
    "        )\n",
    "        d[\"link\"] = url_link.split(\"/shopping/\")[-1] + \"overview\"\n",
    "    except Exception as e:\n",
    "        e[\"link\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            \"link\\n\" + str(e),\n",
    "        ]\n",
    "\n",
    "    # Get car features listed under Basics section\n",
    "    try:\n",
    "        basics_items = soup.find(\n",
    "            \"div\", {\"class\": \"vdp-details-basics\"}\n",
    "        ).find_all(\"li\", {\"class\": \"vdp-details-basics__item\"})\n",
    "        # print(listing_number)\n",
    "        for basics_field in basics_items:\n",
    "            key = basics_field.find(\"strong\").text.replace(\":\", \"\")\n",
    "            value = basics_field.find(\"span\").text.strip()\n",
    "            d[key] = value\n",
    "    except Exception as e:\n",
    "        e[\"link\"] = [f\"page_{page_number}\", f\"listing_{listing_number}\", str(e)]\n",
    "\n",
    "    # Get seller address and zipcode\n",
    "    try:\n",
    "        seller_addr = soup.find(\n",
    "            \"div\", {\"class\": \"seller-details-location\"}\n",
    "        ).text.strip()\n",
    "        d[\"seller_address\"] = seller_addr\n",
    "    except Exception as e:\n",
    "        e[\"seller_address\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "        e[\"seller_zip\"] = [\n",
    "            f\"page_{page_number}\", f\"listing_{listing_number}\", np_nan,\n",
    "        ]\n",
    "    d[\"seller_zip\"] = d[\"seller_address\"].split(state + \" \")[-1]\n",
    "\n",
    "    # Get seller rating and reviews\n",
    "    try:\n",
    "        seller_rat = soup.find(\n",
    "            \"p\", {\"class\": \"rating__link rating__link--has-reviews\"}\n",
    "        ).text.strip()\n",
    "        d[\"seller_rating\"] = seller_rat.split(\")\")[0].replace(\"(\", \"\")\n",
    "        d[\"seller_reviews\"] = (\n",
    "            seller_rat.split(\")\")[-1].replace(\" Reviews\", \"\").strip()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        e[\"seller_rating\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "        e[\"seller_reviews\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            np_nan,\n",
    "        ]\n",
    "\n",
    "    # Get vehicle type (new or old) and title of listing\n",
    "    title_d = soup.find(\"div\", {\"class\": \"vehicle-info__title-container\"})\n",
    "    d[\"type\"] = title_d.find(\"h1\", {\"class\": \"vehicle-info__stock-type\"}).text\n",
    "    d[\"title\"] = title_d.find(\n",
    "        \"h1\", {\"class\": \"cui-heading-2--secondary vehicle-info__title\"}\n",
    "    ).text.strip()\n",
    "\n",
    "    # Get miles driven\n",
    "    try:\n",
    "        miles_cont = soup.find(\n",
    "            \"div\",\n",
    "            {\"class\": \"vdp-cap-price__mileage--mobile vehicle-info__mileage\"},\n",
    "        )\n",
    "        # print(listing_number, miles_cont)\n",
    "        d[\"miles\"] = miles_cont.text if miles_cont else None\n",
    "        # print(listing_number, d[\"miles\"])\n",
    "    except Exception as e:\n",
    "        e[\"miles\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "\n",
    "    # Get dealer price (if listed)\n",
    "    dealer_price = soup.find(\n",
    "        \"div\",\n",
    "        {\"class\": \"vehicle-info__price vehicle-info__price--dealer-price\"},\n",
    "    )\n",
    "    # print(dealer_price)\n",
    "    # print(listing_number, dealer_price.text)\n",
    "    if not dealer_price:\n",
    "        dealer_price = soup.find(\n",
    "            \"span\",\n",
    "            {\n",
    "                \"class\": \"vehicle-info__price-display vehicle-info__price-display--dealer cui-heading-2\"\n",
    "            }\n",
    "        )\n",
    "        if dealer_price:\n",
    "            d[\"dealer_price\"] = dealer_price.text\n",
    "    elif dealer_price.find_all(\"span\"):\n",
    "        dp = dealer_price.find_all(\"span\")\n",
    "        # print(listing_number, dp)\n",
    "        dp_value = [\n",
    "            m.text if \"Dealer Price \" not in m.text else np_nan\n",
    "            for m in dp\n",
    "        ]\n",
    "        # print(listing_number, dp_value)\n",
    "        d[\"dealer_price\"] = next(\n",
    "            (item for item in dp_value if item is not np_nan),\n",
    "            np_nan,\n",
    "        )\n",
    "    # print(listing_number, d[\"dealer_price\"])\n",
    "\n",
    "    # Get MSRP (if listed)\n",
    "    msrp = soup.find(\"div\", {\"class\": \"vehicle-info__price--msrp\"})\n",
    "    # print(listing_number, msrp)\n",
    "    if msrp:\n",
    "        msrp_cont = msrp.find_all(\"span\")\n",
    "        # print(listing_number, msrp_cont)\n",
    "        msrp_value = [\n",
    "            m.text if \"MSRP\" not in m.text else np_nan for m in msrp_cont\n",
    "        ]\n",
    "        d[\"msrp\"] = next(\n",
    "            (item for item in msrp_value if item is not np_nan),\n",
    "            np_nan,\n",
    "        )\n",
    "    # print(listing_number, d[\"msrp\"])\n",
    "\n",
    "    # Get dealer reviews (these are dealer reviews left by other customers)\n",
    "    cons_star = soup.find(\"div\", {\"class\": \"overall-review-stars\"})\n",
    "    d[\"consumer_stars\"] = cons_star.text.strip() if cons_star else np_nan\n",
    "    # print(listing_number, d[\"consumer_stars\"])\n",
    "    cons_reviews = soup.find(\"div\", {\"class\": \"review-stars-average\"})\n",
    "    d[\"consumer_reviews\"] = (\n",
    "        cons_reviews.text.strip()\n",
    "        .replace(\"Average based on\", \"\")\n",
    "        .strip()\n",
    "        .replace(\" reviews\", \"\")\n",
    "        if cons_reviews\n",
    "        else np_nan\n",
    "    )\n",
    "    # print(listing_number, d[\"consumer_reviews\"])\n",
    "\n",
    "    # Get review ratings by category\n",
    "    try:\n",
    "        reviews_ratings = soup.find(\"div\", {\"class\": \"review-rating-breakdown\"})\n",
    "        if reviews_ratings:\n",
    "            rev_cols = reviews_ratings.find_all(\n",
    "                \"div\", {\"class\": \"review-column\"}\n",
    "            )\n",
    "            for rev_col in rev_cols:\n",
    "                rev_rows = rev_col.find_all(\"div\", {\"class\": \"review-row\"})\n",
    "                for r in rev_rows:\n",
    "                    fields_values = r.findAll(\"p\")\n",
    "                    key = \"\"\n",
    "                    stars = \"\"\n",
    "                    for field_value in fields_values:\n",
    "                        if not field_value.find(\"strong\"):\n",
    "                            key = key + field_value.text\n",
    "                        else:\n",
    "                            stars = stars + field_value.find(\"strong\").text\n",
    "                        d[key] = stars\n",
    "    except Exception as e:\n",
    "        e[\"reviews_ratings\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "\n",
    "    #     print(listing_number)\n",
    "    #     for k, v in d.items():\n",
    "    #         print(k, v)\n",
    "    #     print(\"\\n\")\n",
    "\n",
    "    # Get APR\n",
    "    try:\n",
    "        apr_matches_type1 = re.findall(\n",
    "            \"months at \\d+\\.\\d+% APR\", soup.prettify()\n",
    "        )\n",
    "        apr_matches_type2 = re.findall(\"\\d+\\.\\d+% APR for\", soup.prettify())\n",
    "        matches = (\n",
    "            apr_matches_type1 + apr_matches_type2\n",
    "            if not apr_matches_type1\n",
    "            else apr_matches_type1\n",
    "        )\n",
    "        # print(matches)\n",
    "        d[\"APR\"] = min(re.findall(\"\\d+\\.\\d+\", \", \".join(matches)))\n",
    "        # print(listing_number, d[\"APR\"], len(set(matches)))\n",
    "    except Exception as e:\n",
    "        e[\"APR\"] = [f\"page_{page_number}\", f\"listing_{listing_number}\", str(e)]\n",
    "\n",
    "    # Get lowest per month financing option\n",
    "    # # First type of element location\n",
    "    try:\n",
    "        pmonth_match_1 = soup.find_all(\n",
    "            \"div\", {\"class\": \"online-shopper-v2-payments__price\"}\n",
    "        )\n",
    "        # print(pmonth_match_1)\n",
    "        per_month1 = []\n",
    "        if isinstance(pmonth_match_1, list):\n",
    "            for pmonth_match1 in pmonth_match_1:\n",
    "                # print(p)\n",
    "                p_val = pmonth_match1.text.replace(\"\\n\", \"\").replace(\"$\", \"\")\n",
    "                if p_val != \"\":\n",
    "                    per_month1.append(int(p_val))\n",
    "                else:\n",
    "                    per_month1.append(None)\n",
    "        else:\n",
    "            per_month1 = [None]\n",
    "    except Exception as e:\n",
    "        e[\"per_month_min_method1\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "    # print(i, per_month1)\n",
    "    # # Second type of element location\n",
    "    try:\n",
    "        pmonth_match_2 = soup.find(\n",
    "            \"span\", {\"class\": \"cui-heading-1 monthly-payment\"}\n",
    "        )\n",
    "        pmonth_match_2 = (\n",
    "            int(pmonth_match_2.text.replace(\"$\", \"\"))\n",
    "            if pmonth_match_2\n",
    "            else None\n",
    "        )\n",
    "        # # Combine two types of elements\n",
    "        pmonth_combined = [\n",
    "            x for x in per_month1 + [pmonth_match_2] if x is not None\n",
    "        ]\n",
    "        # print(pmonth_combined)\n",
    "        # # Get minimum of both types of elements to use as per month financing\n",
    "        pmonth_final = min(pmonth_combined) if pmonth_combined else None\n",
    "        # print(i, pmonth_combined, pmonth_final)\n",
    "        d[\"per_month_min\"] = pmonth_final\n",
    "    except Exception as e:\n",
    "        e[\"per_month_min_method2\"] = [\n",
    "            f\"page_{page_number}\",\n",
    "            f\"listing_{listing_number}\",\n",
    "            str(e),\n",
    "        ]\n",
    "    return d, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"web-scraping\"></a>\n",
    "\n",
    "## 2. [Web Scraping](#web-scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now perform the web-scraping and save results to a separate file per page of listings scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for listing in range(listing_number, listing_number + 1):\n",
    "    print(listing)\n",
    "    \n",
    "    if html_file_path.is_file():\n",
    "        with open(str(html_file_path)) as f:\n",
    "            soup = BeautifulSoup(f.read(), 'html.parser')\n",
    "        # print(soup.prettify())\n",
    "\n",
    "        if html_file_path.is_file():\n",
    "            d_listing, d_errors = scrape_single_listing(\n",
    "                soup=soup,\n",
    "                page_number=page_number,\n",
    "                listing_number=listing,\n",
    "            )\n",
    "\n",
    "        df_listing = pd.DataFrame.from_dict(d_listing, orient=\"index\").T\n",
    "        dfe = pd.DataFrame.from_dict(d_errors, orient=\"index\").T\n",
    "        df = pandas_clean_data(df_listing)\n",
    "        df = pandas_rearrange_columns(df)\n",
    "        if not dfe.empty:\n",
    "            dfe[['page', 'listing', \"error\"]] = pd.DataFrame(\n",
    "                dfe[\"error\"].values.tolist(),\n",
    "                index=dfe.index\n",
    "            )\n",
    "            display(dfe)\n",
    "        # display(df)\n",
    "\n",
    "        header_spec = True if listing == 1 else False\n",
    "        df.to_csv(\n",
    "            path_or_buf=fpath / f\"p{page_number}.csv\",\n",
    "            mode=\"a\",\n",
    "            header=header_spec,\n",
    "            index=False,\n",
    "        )\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compare-saved-and-scraped-data\"></a>\n",
    "\n",
    "## 3. [Compare saved and scraped data](#compare-saved-and-scraped-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as a sanity check, we compare the first 5 rows of\n",
    "- vertically concatenated `DataFrame`s of combined scraped data\n",
    "- the first `DataFrame` of scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.concat(dfs, axis=0)\n",
    "dfr = pd.read_csv(fpath / f\"p{page_number}.csv\")\n",
    "display(dfc.head())\n",
    "display(dfr.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These match, as expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
