{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scrape car listing details](#scrape-car-listing-details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size=\"5\">**Important Note**</font>\n",
    "- this notebook does **not** support scraping more than 1 row of IDs from `data/Listings_IDs.txt` at a time\n",
    "- if `num_pages_of_results`, in the last cell of section 1., is set to a value larger than 1, then the behavior of this notebook will be unreliable\n",
    "- this notebook does **not** support Cell > Run All\n",
    "- please run cells manually and wait for the preceding page to load before executing the second last cell before section 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from random import randint, sample\n",
    "from time import sleep, time\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import src.listing_scraper as lsc\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from numpy import nan as np_nan\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set options for the Chrome webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"--headless\") # Runs Chrome in headless mode.\n",
    "options.add_argument('--no-sandbox') # Bypass OS security model\n",
    "options.add_argument('--disable-gpu')  # applicable to windows os only\n",
    "options.add_argument('start-maximized') # \n",
    "options.add_argument('disable-infobars')\n",
    "options.add_argument(\"--disable-extensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set display options for `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the `UserAgent` class in order to create a fake user agent when scraping with Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Use saved IDs to scrape car listings](#use-saved-ids-to-scrape-car-listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will scrape car listing details from the webpage corresponding to `cars.com` listing IDs that were previously retrieved from `1_cars_conroller.ipynb` and stored in the file `data/Listings_IDs.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define two input variables\n",
    "\n",
    "1. `n_listings_per_page`\n",
    "   - this is the number of IDs from each row of `data/Listings_IDs.txt` to be scraped\n",
    "\n",
    "2. `start_listing_num` is the starting number for the IDs from `data/Listings_IDs_YYYmmdd.txt` to be scraped\n",
    "   - the maximum value for `start_listing_num` (for each city) is explained below based on row numbers from `data/ListingsIDs.txt`\n",
    "     - AUS\n",
    "       - 30 pages of 100\n",
    "       - listings 0-2999\n",
    "       - `min(start_listing_num) = 0`\n",
    "       - `max(start_listing_num) = 2900`\n",
    "     - SEA\n",
    "       - 11 pages of 100\n",
    "       - listings 3000-4099\n",
    "       - `min(start_listing_num) = 3000`\n",
    "       - `max(start_listing_num) = 4000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_listings_per_page = 100  # max = 100\n",
    "start_listing_num = 1100  # min = 0 (must be 0 or multiple of 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other inputs we'll define below should not be modified by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path to Chrome driver\n",
    "chromedriver = \"./chromedriver\"\n",
    "\n",
    "# Format for listing URL with placeholder for listing ID\n",
    "url_base = \"https://www.cars.com/vehicledetail/detail/{}/overview/\"\n",
    "\n",
    "# Path to folder where filtered listing IDs should be stored\n",
    "fpath = Path().cwd() / \"data\"\n",
    "\n",
    "# Path to file where filtered listing IDs should be stored\n",
    "# ids_filename = str(fpath / \"Listings_IDs_20191008_AUS_only.txt\")\n",
    "ids_filename = str(fpath / \"Listings_IDs_20191009_SEA_only.txt\")\n",
    "\n",
    "# Dictionary of xpaths for elements that should randomly be brought into view on the listing page\n",
    "page_element_xpath_strings = {\n",
    "    \"All Features\": \"//h4[@class='vdp-details-basics__features page-section__title--sub cui-heading-2']\",\n",
    "    \"Have a question?\": \"//h3[@class='cui-heading-3']\",\n",
    "    \"Request an Appointment\": \"//a[@data-linkname='email-lead-form-test-drive-bottom']\",\n",
    "    \"Seller's Notes (See more)\": \"//label[@data-linkname='expand-seller-notes']\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We programmatically determine the state based on the number of listing IDs scraped for each zipcode\n",
    "- details are shown above under the explanation for the variable `start_listing_num`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = \"WA\" if start_listing_num <= 2900 else \"TX\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will specify the number of rows of listing IDs from `data/Listings_IDs.txt` that will be scraped during one full run of this notebook. This variable should always be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages_of_results = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use-saved-ids-to-scrape-car-listings\"></a>\n",
    "\n",
    "## 2. [Use saved IDs to scrape car listings](#use-saved-ids-to-scrape-car-listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `1_cars_controller.ipynb`, we submitted inputs to the user submission form on `cars.com` and then scraped the ID of search results for listings. \n",
    "\n",
    "Here, we will scrape individual search result listings by using those previously saved IDs to assemble a URL for the associated listing. The scraped results will be appended to a `*.csv` file - one file per listing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by loading the file of scraped listing `ID`s. We will use these to assemble the URL for the listing, by pre-pending the base url string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pause_code(min_time, max_time, delay_msg):\n",
    "    \"\"\"Wait for a random amount of time before proceeding\"\"\"\n",
    "    # Pause\n",
    "    delay_time = randint(min_time, max_time)\n",
    "    print(delay_msg)\n",
    "    sleep(delay_time)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define variables to track\n",
    "- the first and last required listing\n",
    "- page number\n",
    "- (randomly chosen) listings at which to scroll to the bottom of the page\n",
    "- etc.\n",
    "\n",
    "To explain how these are calculated, assume `start_listing_num = 400`\n",
    "\n",
    "1. `page_num` and `page_start_listing_num`\n",
    "   - Based on the user's input for `start_listing_num` above, we will `divmod` obtain the\n",
    "     - starting listing number on the appropriate page\n",
    "     - page number (zero-indexed)\n",
    "   - For each page of listings returned from the filters applied to the `cars.com` homepage, the previous notebook `1_cars_controller.ipynb` had exported 100 listing IDs to `data/Listings_IDs.txt`. In the current notebook, if the user enters `start_listing_num = 400`, this will correspond to the\n",
    "     - 400th listing ID which is the 0th ID on row 5 of `data/Listing_IDs.txt`\n",
    "       - 0 is assigned to `page_start_listing_num`\n",
    "     - 5th page of search results returned from the `cars.com` homepage\n",
    "       - 5 is assigned to `page_num`\n",
    "2. `page_end_listing_num`\n",
    "   - this is just `page_start_listing_num` (0) + `n_listings_per_page` (100)\n",
    "3. `element`\n",
    "   - this is a list of all IDs on the required row (row 5)\n",
    "4. `id_list`\n",
    "   - this comes from splitting the requited row (row 5), which is a string of comma-separated IDs, into a list of strings\n",
    "5. `listings_to_move`\n",
    "   - this is a list of randomly selected listings at which `selenium` will scroll to the bottom of the page\n",
    "   - at all other listings, `selenium` will successively [scroll into view](https://stackoverflow.com/a/50288690/4057186) three separate elements on the listing page (if those elements are present)\n",
    "6. `out_fpath`\n",
    "   - this is the name of the `*.csv` file to which scraped listing details will be exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ids_filename) as f:\n",
    "    # Read lines in file with listing IDs\n",
    "    # - one line per page of returned search results that were saved\n",
    "    #   to a *.txt file\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Get the (a) preceding page number and (b) first listing number\n",
    "# on the required page\n",
    "page_num, page_start_listing_num = divmod(start_listing_num, 100)\n",
    "\n",
    "# Get the last listing number on the required page (upper bound exclusive)\n",
    "page_end_listing_num = n_listings_per_page + page_start_listing_num\n",
    "\n",
    "# Get all listings on required page number\n",
    "element = lines[page_num]\n",
    "\n",
    "# Split line string to create a list and slice to get only the required listing IDs\n",
    "id_list = element.split(\", \")[page_start_listing_num: page_end_listing_num]\n",
    "\n",
    "# Randomly select ID index at which to generate scroll with\n",
    "# selenium on the corresponding listing page\n",
    "listings_to_move = sample(\n",
    "    range(page_start_listing_num, page_end_listing_num),\n",
    "    int(n_listings_per_page / 3)\n",
    ") if n_listings_per_page >=3 else [None]\n",
    "\n",
    "# Assemble path to output file that will be produced\n",
    "out_fpath = (\n",
    "        fpath / (\n",
    "            f\"p{page_num}__\"\n",
    "            f\"{page_start_listing_num}_\"\n",
    "            f\"{page_end_listing_num - 1}.csv\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll summarize the above variables in a pandas `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"Required number of listings per page\": n_listings_per_page,\n",
    "    \"Overall first listing number required\": start_listing_num,\n",
    "    \"Maximum number of listings available\": (30 + 11) * 100,\n",
    "    \"Maximum pages available\": 30 + 11,  # AUS zipcode: 30, SEA zipcode: 11\n",
    "    \"State to scrape\": state,\n",
    "    \"Page number selected\": page_num,\n",
    "    \"First selected listing number\": page_start_listing_num,\n",
    "    \"Required total number of listings\": n_listings_per_page,\n",
    "    \"Last selected listing number (upper-bound inclusive)\": page_end_listing_num - 1,\n",
    "    \"Scrolling to bottom of page for listing numbers\": f\"{', '.join(str(x) for x in listings_to_move)}\",\n",
    "    \"Output *.csv filepath\": out_fpath,\n",
    "}\n",
    "display(pd.DataFrame.from_dict(d, orient=\"index\").reset_index(drop=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each listing ID in `data/Listings_IDs.txt`, we'll do the following\n",
    "1. Assemble URL to the listing web page on `cars.com`\n",
    "2. Load the listing webpage\n",
    "3. get the `bs4` soup\n",
    "4. Randomly do one of the following\n",
    "   - scroll to bottom of page\n",
    "   - first: pause for a random amount of time\n",
    "   - second: successively bring three page elements into view (if they are found on the page)\n",
    "     - the `xpath` search string for each of these elements is stored as values in the earlier defined dictionary `page_element_xpath_strings`\n",
    "5. Close the active browser window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_st = time()\n",
    "header_check = []\n",
    "\n",
    "# Loop over list of IDs and scrape the associated listings\n",
    "for link_cntr, eid in enumerate(id_list):\n",
    "    start_time = time()\n",
    "    # 1. Assemble listing url from ID\n",
    "    listing_url = f\"{url_base}\".format(eid.replace(\"\\n\", \"\"))\n",
    "    print(\n",
    "        f\"Page: {page_num}, \"\n",
    "        f\"Listing: {link_cntr + start_listing_num}, \"\n",
    "        f\"URL: {listing_url}\"\n",
    "    )\n",
    "\n",
    "    # Instantiate a random user agent\n",
    "    userAgent = ua.random\n",
    "    # print(userAgent)\n",
    "    options.add_argument(f\"user-agent={userAgent}\")\n",
    "    \n",
    "    # Instantiate Chrome webdriver with the above random user agent\n",
    "    driver = webdriver.Chrome(\n",
    "        options=options, executable_path=str(chromedriver)\n",
    "    )\n",
    "\n",
    "    # 2. Load web page\n",
    "    driver.get(listing_url)\n",
    "\n",
    "    try:\n",
    "        # 3. Scrape web page and append to one *.csv per page\n",
    "        soup_contents = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        sold_check = soup_contents.find(\"div\", {\"class\": \"vdp__no-listing__alert\"})\n",
    "        if sold_check and \"No longer listed\" in sold_check.text:\n",
    "            print(\"Sold, car is no longer listed. Will skip listing...\\n\")\n",
    "            header_check.append(link_cntr)\n",
    "        else:\n",
    "            link_cntr = (link_cntr - 1) if header_check and header_check[0] == 0 else link_cntr\n",
    "            d_listing, d_errors = lsc.scrape_single_listing(\n",
    "                soup=soup_contents,\n",
    "                page_number=page_num,\n",
    "                listing_number=link_cntr + start_listing_num,\n",
    "                state=state,\n",
    "            )\n",
    "\n",
    "            # Put errors and listings into DataFrame\n",
    "            df_listing = pd.DataFrame.from_dict(d_listing, orient=\"index\").T\n",
    "            dfe = pd.DataFrame.from_dict(d_errors, orient=\"index\").T\n",
    "            df = lsc.pandas_clean_data(df_listing)\n",
    "            if not dfe.empty:\n",
    "                dfe[[\"page\", \"listing\", \"error\"]] = pd.DataFrame(\n",
    "                    dfe[\"error\"].values.tolist(),\n",
    "                    index=dfe.index\n",
    "                )\n",
    "                display(dfe)\n",
    "            # display(df)\n",
    "            header_spec = True if (link_cntr + 1) == 1 else False\n",
    "            # Append DataFrame to *.csv\n",
    "            print(\"Writing header to output *.csv file?\", header_spec)\n",
    "            df.to_csv(\n",
    "                path_or_buf=str(out_fpath),\n",
    "                mode=\"a\",\n",
    "                header=header_spec,\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "            # 3. Randomly perform of the following 2 actions on the page\n",
    "            #    - scroll to bottom of page\n",
    "            #    - bring one of 3 pre-selected sections of the page into view\n",
    "            #      (if element is found on page)\n",
    "            if (link_cntr + start_listing_num) in listings_to_move:\n",
    "                # (a) Scroll to bottom of page\n",
    "                print(\n",
    "                    f\"Moving to bottom of page for listing number {link_cntr + start_listing_num}\"\n",
    "                )\n",
    "                driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "                )\n",
    "                print(f\"Reached to the bottom of the page\")\n",
    "            else:\n",
    "                # (b) Bring one of 3 pre-selected elements into view\n",
    "                #     (if element is found on page)\n",
    "                a = 1\n",
    "                for (\n",
    "                    element_name,\n",
    "                    element_string_xpath,\n",
    "                ) in page_element_xpath_strings.items():\n",
    "                    # Pause for random time delay\n",
    "                    pause_code(\n",
    "                        min_time=0,  # 3\n",
    "                        max_time=2,  # 10\n",
    "                        delay_msg=(\n",
    "                            f\"Pause between bringing {element_name} \"\n",
    "                            \"page element into view\"\n",
    "                        ),\n",
    "                    )\n",
    "                    # Bring element into view (if element is found on page)\n",
    "                    try:\n",
    "                        element = driver.find_element_by_xpath(\n",
    "                            element_string_xpath\n",
    "                        )\n",
    "                        driver.execute_script(\n",
    "                            \"arguments[0].scrollIntoView();\", element\n",
    "                        )\n",
    "                    except NoSuchElementException as e:\n",
    "                        print(\n",
    "                            f\"Page: {page_num}, Listing: {link_cntr + start_listing_num} \"\n",
    "                            + str(e)\n",
    "                        )\n",
    "            # 4. Close active web browser window\n",
    "            driver.close()\n",
    "\n",
    "            print(f\"Leaving page {page_num} listing {link_cntr + start_listing_num}\")\n",
    "            elapsed_time = time() - start_time\n",
    "            print(\n",
    "                f\"Time spent on page {page_num} \"\n",
    "                f\"listing {link_cntr + start_listing_num} = {elapsed_time:.2f} seconds\\n\"\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "total_minutes, total_seconds = divmod(time() - cell_st, 60)\n",
    "print(f\"Cell exection time: {int(total_minutes):d} minutes, {total_seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll print all the exported listing details in a pandas `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_csv(str(out_fpath))\n",
    "display(df_loaded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
