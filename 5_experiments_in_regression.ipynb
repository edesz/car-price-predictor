{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Regression trials](#regression-trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from collections import namedtuple\n",
    "from IPython.display import display, display_html\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import (\n",
    "    cross_validate,\n",
    "    GridSearchCV,\n",
    "    KFold,\n",
    "    PredefinedSplit,\n",
    "    RepeatedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    PolynomialFeatures,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    StandardScaler,\n",
    ")\n",
    "from yellowbrick.model_selection import LearningCurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport src.visualization_helpers\n",
    "from src.visualization_helpers import (\n",
    "    plot_corr_map,\n",
    "    plot_coef_plot,\n",
    "    plot_cv_scores,\n",
    "    plot_nested_cv,\n",
    "    plot_multiple_histograms,\n",
    "    plot_pairplot,\n",
    "    plot_qq,\n",
    "    plot_residual_manually,\n",
    "    show_yb_learning_curves,\n",
    "    show_yb_prediction_error,\n",
    "    show_yb_residual_plot,\n",
    "    plot_multi_feature_target,\n",
    "    plot_single_feature_target,\n",
    "    plot_multiple_bar_charts,\n",
    "    plot_side_by_side_bar_box,\n",
    ")\n",
    "\n",
    "%aimport src.metrics_helpers\n",
    "from src.metrics_helpers import get_test_metrics\n",
    "\n",
    "%aimport src.ml_helpers\n",
    "from src.ml_helpers import (\n",
    "    generate_dummies,\n",
    "    filter_by_value_counts,\n",
    "    get_model_coefs,\n",
    "    DFDropDupicates,\n",
    "    DFFilterNumerical,\n",
    "    DFPolynomialFeatureGenerator,\n",
    "    DFInteractionTerms,\n",
    "    DFDropDupicates,\n",
    "    DFDropNa,\n",
    "    DFColTypeChanger,\n",
    "    DFOneHotEncoder,\n",
    "    DFStandardScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 22\n",
    "BIGGER_SIZE = 24\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "sns.set_style(\"darkgrid\", {\"legend.frameon\": False})\n",
    "sns.set_context(\"talk\", font_scale=0.95, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 750)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "\n",
    "## [Table of Contents](#table-of-contents)\n",
    "0. [About](#about)\n",
    "1. [User Inputs](#user-inputs)\n",
    "2. [Load Price listing data and split](#load-price-listing-data-and-split)\n",
    "3. [Exploratory Data Analysis on target variable](#exploratory-data-analysis-on-target-variable)\n",
    "4. [Applying filters to drop outliers](#applying-filters-to-drop-outliers)\n",
    "5. [Pre-processing](#pre-processing)\n",
    "6. [Preparing for modeling and feature selection](#preparing-for-modeling-and-feature-selection)\n",
    "7. [Base model - numerical features only](#base-model-numerical-features-only)\n",
    "   - 7.1. [Raw features](#raw-features)\n",
    "   - 7.2. [Transformed features](#transformed-features)\n",
    "     - 7.2.1. [Quantile transformation](#quantile-transformation)\n",
    "     - 7.2.2. [Yeo-Johnson transformation](#yeo-johnson-transformation)\n",
    "     - 7.2.3. [`log1plus` transformation](#log1plus-transformation)\n",
    "   - 7.3. [Polynomial and Interaction terms](#polynomial-and-interaction-terms)\n",
    "8. [Include categorical features](#include-categorical-features)\n",
    "   - 8.1. [Best categorical features](#best-categorical-features)\n",
    "   - 8.2. [Summary of adding categorical features](#summary-of-adding-categorical-features)\n",
    "9. [Assessing best model](#assessing-best-model)\n",
    "   - 9.1. [Extracting best model](#extracting-best-model)\n",
    "   - 9.2. [Get best model coefficients](#get-best-model-coefficients)\n",
    "   - 9.3. [Residuals for predictions](#residuals-for-predictions)\n",
    "     - 9.3.1. [Residual distributions](#residual-distributions)\n",
    "     - 9.3.2. [Worst performing predictions](#worst-performing-predictions)\n",
    "     - 9.3.3. [Residuals separated by State](#residuals-separated-by-state)\n",
    "     - 9.3.4. [Quantile-quantile plot](#quantile-quantile-plot)\n",
    "   - 9.4. [Permutation Importances](#permutation-importances)\n",
    "   - 9.5. [Prediction error](#prediction-error)\n",
    "   - 9.6. [Summary of scoring metrics](#summary-of-scoring-metrics)\n",
    "     - 9.6.1. [Comparison of ordinary linear regression between `sklearn` and `statsmodels`](#comparison-of-ordinary-linear-regression-between-sklearn-and-statsmodels)\n",
    "       - 9.6.1.1. [Using `statsmodels`](#using-statsmodels)\n",
    "       - 9.6.1.2. [Using `sklearn`](#using-sklearn)\n",
    "   - 9.7. [Check influence of choice of test split](#check-influence-of-choice-of-test-split)\n",
    "   - 9.8. [Check of bias and variance](#check-of-bias-and-variance)\n",
    "10. [Future work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"about\"></a>\n",
    "\n",
    "## 0. [About](#about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment with regression models on the processed car listings data in `data/processed_*.csv`. As mentioned in the `README.md` file, the objective is to build a linear regression model that can use listed features about cars listed to predict the listing price to within \\$5,000.\n",
    "\n",
    "**Notes**\n",
    "1. Throughout this analysis, we will refer to the requirements of linear regression, which can be found in this [Duke guide](http://people.duke.edu/~rnau/testing.htm).\n",
    "2. This analysis will be limited to the following (`v1`, version 1) set of columns from the data (at the most)\n",
    "   - `Mileage`\n",
    "   - `MPG`\n",
    "   - `consumer_reviews`\n",
    "   - `seller_reviews`\n",
    "   - `Fuel Type`\n",
    "   - `Drivetrain`\n",
    "   - `State`\n",
    "   - `seller_rating`\n",
    "   - `consumer_stars`\n",
    "   - `Comfort`\n",
    "   - `Performance`\n",
    "   - `Exterior Styling`\n",
    "   - `Interior Design`\n",
    "   - `Reliability`\n",
    "   - `year`\n",
    "   - `make`\n",
    "   - `trans_speed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"user-inputs\"></a>\n",
    "\n",
    "## 1. [User Inputs](#user-inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define lists containing the names of features to be used in the regression modelling. Separate lists will be created for numerical and categorical features\n",
    "- some are commented as engineered feature, and will be discussed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Numerical columns\n",
    "nums = [\n",
    "    \"Mileage\",\n",
    "    \"MPG\",  # \"Highway MPG\" + \"City MPG\",\n",
    "    \"consumer_reviews\",\n",
    "    \"seller_reviews\",\n",
    "    # \"tank_volume\",  # higher RMSE when included instead of MPG; correlated to MPG, so can't combine\n",
    "]\n",
    "\n",
    "# Categorical columns\n",
    "cats = [\n",
    "    \"Fuel Type\",\n",
    "    \"Drivetrain\",\n",
    "    #     # \"Exterior Color\",  # very high cardinality, so exclude\n",
    "    #     # \"Interior Color\",  # very high cardinality, so exclude\n",
    "    \"State\",  # consider this to be a proxy for [\"seller_zip\" + \"City\"]\n",
    "    #     # \"seller_rating\",  # RMSE on validation set blows up for LinearRegression\n",
    "    #     # \"consumer_stars\",  # RMSE on validation set blows up for LinearRegression\n",
    "    \"Comfort\",  # nested CV and learning curves blow up for LinearRegression\n",
    "    #     # \"Performance\",  # slightly higher RMSE when this is included, so exclude\n",
    "    \"Exterior Styling\",  # nested CV and learning curves blow up for LinearRegression\n",
    "    \"Interior Design\",  # nested CV and learning curves blow up for LinearRegression\n",
    "    #     # \"Reliability\",  # slightly higher RMSE when this is included, so exclude\n",
    "    \"year\",\n",
    "    #     # \"model\",  # very high cardinality, so exclude\n",
    "    \"trans_speed\",\n",
    "]\n",
    "\n",
    "# Number of inner and outer trials for nested cross-validation\n",
    "NUM_NESTED_CV_TRIALS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_selected = nums + cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following dictionaries are not used in analysis here\n",
    "# # Dictionary to define feature interactions for custom pipeline transformers\n",
    "interact = {\n",
    "    \"mpg05Xtank_volume\": {\"f1\": \"MPG05\", \"f2\": \"tank_volume\"},\n",
    "    \"mileageXtank_volume\": {\"f1\": \"Mileage\", \"f2\": \"tank_volume\"},\n",
    "    \"mpg05Xconsumer_reviews\": {\"f1\": \"MPG05\", \"f2\": \"consumer_reviews\"},\n",
    "}\n",
    "# # Dictionary to define filters for categorical columns for custom pipeline transformers\n",
    "vc = {\n",
    "    \"vc_filter_fuel_type\": {\n",
    "        \"col\": \"Fuel Type\",\n",
    "        \"numerical_value\": 10,\n",
    "        \"filter_type\": \"vc_lt\",\n",
    "    },\n",
    "    \"vc_filter_trans_speed\": {\n",
    "        \"col\": \"trans_speed\",\n",
    "        \"numerical_value\": 10,\n",
    "        \"filter_type\": \"vc_lt\",\n",
    "    },\n",
    "    \"vc_filter_make\": {\"col\": \"make\", \"numerical_value\": 10, \"filter_type\": \"vc_lt\"},\n",
    "    \"vc_filter_seller_rating\": {\n",
    "        \"col\": \"seller_rating\",\n",
    "        \"numerical_value\": [3.8, 3.7, 4.0],\n",
    "        \"filter_type\": \"vc_isin\",\n",
    "    },\n",
    "    \"vc_filter_consumer_stars\": {\n",
    "        \"col\": \"consumer_stars\",\n",
    "        \"numerical_value\": 10,\n",
    "        \"filter_type\": \"vc_lt\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_scorer(y_true, y_pred):\n",
    "    return -1 * mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-price-listing-data-and-split\"></a>\n",
    "\n",
    "## 2. [Load Price listing data and split](#load-price-listing-data-and-split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by loading the car listings price data into a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_storage_container_name = \"myconedesx7\"\n",
    "az_blob_name = \"blobedesz17\"\n",
    "conn_str = (\n",
    "    \"DefaultEndpointsProtocol=https;\"\n",
    "    f\"AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};\"\n",
    "    f\"AccountKey={os.getenv('AZURE_STORAGE_KEY')};\"\n",
    "    f\"EndpointSuffix={os.getenv('ENDPOINT_SUFFIX')}\"\n",
    ")\n",
    "blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "blob_client = blob_service_client.get_blob_client(\n",
    "    container=az_storage_container_name, blob=az_blob_name\n",
    ")\n",
    "blobstring = blob_client.download_blob().content_as_text()\n",
    "df = pd.read_csv(StringIO(blobstring))\n",
    "display(df.head(2))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will now be split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"price\"], axis=1)\n",
    "y = df[\"price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "train_indices = X_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are approximately 500 more records for `TX` than for `WA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"State\"].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploratory-data-analysis-on-target-variable\"></a>\n",
    "\n",
    "## 3. [Exploratory Data Analysis on target variable](#exploratory-data-analysis-on-target-variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start exploration of the dataset, the distribution of the target variable - the listing price - is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_train)\n",
    "fig = plt.gcf()\n",
    "ax.set_title(\"Histogram of listing prices\", loc=\"left\", fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Although generally shaped similar to a normal distribution, we observe stronger (higher) tails and broader width for than for a normal distribution. Also there is a left skew that could introduce non-linearities - taking a `log` would have been beneficial for a right skew, but would only further skew an already existing left skew as is the case here.\n",
    "2. A weak bimodal shape appears in the distribution. The two apparent peaks centered at approx. USD 25,000 and USD 37,000 appear to be partly merged together creating this effect. We could filter the data based on this to remove the bimodal features, but this would introduce selection bias since we don't know the test set distribution or, more importantly, the distribution of data to be used for inference (if such a model were to be considered in a tool put into production). Instead, the raw `price` target data will be comapred against transformed versions, in linear regression models, and the best transformation or the untransformed version of the data (based on lowest cross-validation error score) will be used. This could also be further investigated by scraping more data to determine if the two apparent peaks can be more clearly separated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"applying-filters-to-drop-outliers\"></a>\n",
    "\n",
    "## 4. [Applying filters to drop outliers](#applying-filters-to-drop-outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first de-select rows where the numerical feature `per_month_min` is zero. These correspond to listings where the monthly payment is listed as zero dollars which is unrealistic. The relationship between these two `price`-based features (`per_month_min` and `price`) is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_zero_pmm_idx = X_train[X_train[\"per_month_min\"] != 0].index\n",
    "plot_corr_map(\n",
    "    df=pd.concat(\n",
    "        [y_train.loc[non_zero_pmm_idx], X_train.loc[non_zero_pmm_idx, \"per_month_min\"]],\n",
    "        axis=1,\n",
    "    ),\n",
    "    annot=True,\n",
    "    annot_fmt=\".3f\",\n",
    "    fig_size=(2, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. It is clear that the `per_month_min` financing option is determined from the listing `price`. So, is a perfect correlation to price. So we will exclude this feature in our models in order to avoid the problem of multi-collinearity in linear regression when interpreting linear model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply two filters to the numerical features `MPG` and `Mileage`, in the training data, in order to remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered = X_train[(X_train[\"MPG\"] < 2000) & (X_train[\"Mileage\"] < 10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now visualize the relationship between the numerical features and the target `price` in the filtered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_filtered = pd.concat([X_train_filtered, y_train], axis=1)\n",
    "nums_all = [\"Mileage\", \"MPG\", \"consumer_reviews\", \"seller_reviews\", \"tank_volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_feature_target(\n",
    "    df_train_filtered,\n",
    "    [\"MPG\", \"price\", \"State\", \"make\", \"year\"],\n",
    "    [\"price\"],\n",
    "    nums_all,\n",
    "    label_font_size=12,\n",
    "    title_font_size=12,\n",
    "    marker_size=80,\n",
    "    marker_opacity=0.5,\n",
    "    marker_linewidth=0.5,\n",
    "    color_by_col=\"State\",\n",
    "    legend_vertical_offset=-5,\n",
    "    fig_size=(250, 300),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Only one numerical feature `MPG` has a reasonable linear relationship to the target `price`\n",
    "   - it requires further filtering to remove outliers, about approx. 80 MPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at bar charts of all the categorical features chosen for this workflow ([version 1](#about))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_cols_bar_chart = [\n",
    "    \"Fuel Type\",\n",
    "    \"Drivetrain\",\n",
    "    \"State\",\n",
    "    \"seller_rating\",\n",
    "    \"consumer_stars\",\n",
    "    \"Comfort\",\n",
    "    \"Performance\",\n",
    "    \"Exterior Styling\",\n",
    "    \"Interior Design\",\n",
    "    \"Reliability\",\n",
    "    \"year\",\n",
    "    \"make\",\n",
    "    \"trans_speed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_bar_charts(\n",
    "    df_train_filtered, cats_cols_bar_chart, 20, 3, -3, 12, 12, (300, 350)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. `Fuel Type` is dominated by `Gasoline` cars, with `Hybrid`s three orders of magnitude smaller. We could drop the non-`Gasoline` options from here.\n",
    "2. `Comfort`, `Performance`, `Exterior Styling`, `Interior Design` and `Reliability` have 3-, 4- and 5-star rating entries; if we consider a poor rating to be lower than 2.5 stars, then we only have better than average (2.5 stars) rating cars for all these categories and we don't necessarily have a clear choice to filter these columns\n",
    "3. There are more listings in `TX` than `WA`\n",
    "4. `make`, `seller_rating` and `consumer_stars` are high-cardinality features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the influence of each of these categorical features on the main numerical feature (`MPG`, which visually showed the strongest linear trend with the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_feature_target(\n",
    "    df_train_filtered,\n",
    "    \"MPG\",\n",
    "    \"price\",\n",
    "    [\"MPG\", \"price\", \"State\", \"make\", \"year\"],\n",
    "    cats_cols_bar_chart,\n",
    "    12,\n",
    "    12,\n",
    "    60,\n",
    "    1,\n",
    "    0.5,\n",
    "    \"category20\",\n",
    "    title_horizontal_offset=45,\n",
    "    title_vertical_offset=-3,\n",
    "    fig_size=(250, 300),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. When applying the second filter to the `MPG` column, say at 80 MPG, to remove the outlying points from the linear grouping, we would notably give up\n",
    "   - all `Hybrid`s and `Electric` (`Fuel Type`) vehicles\n",
    "     - these splot distinctly separate from `Gasoline`\n",
    "   - some `1` (single)-speed transmission cars\n",
    "\n",
    "This filter is required into to improve the linear relationship between `MPG` and the target `price` column in the data, since none of the other scraped numerical features have even a visible linear trend which is a requirement for a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "\n",
    "## 5. [Pre-processing](#pre-processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a pipeline object with custom transformers to transform (filter, change dtypes, one-hot encode) the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # (\"duplicates\", DFDropDupicates(keep=\"first\")),\n",
    "        (\"filter_mpg\", DFFilterNumerical(\"MPG\", 80, \"lt\")),\n",
    "        (\"filter_mileage\", DFFilterNumerical(\"Mileage\", 10000, \"lt\")),\n",
    "        (\"nan\", DFDropNa(cols=all_selected, how=\"any\")),\n",
    "        (\"str_dtype\", DFColTypeChanger([\"consumer_stars\", \"seller_rating\"], \"str\")),\n",
    "        (\"ohe\", DFOneHotEncoder(cats, \"=\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use this pipeline to transform the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_transformer.fit(X_train)\n",
    "# print(pipe_transformer.transform(X_train).shape)\n",
    "# print(pipe_transformer.transform(X_test).shape)\n",
    "X_train = pipe_transformer.fit_transform(X_train)\n",
    "X_test = pipe_transformer.transform(X_test)\n",
    "y_train = y_train.loc[X_train.index]\n",
    "y_test = y_test.loc[X_test.index]\n",
    "print(X_train.shape)\n",
    "print(X_train.shape, len(y_train))\n",
    "print(X_test.shape, len(y_test))\n",
    "display(X_train.head(2))\n",
    "display(X_train.loc[:, X_train.columns.duplicated()].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummified_cols_flat = list(X_train.filter(regex=\"=\"))\n",
    "print(len(dummified_cols_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll summarize the transformed train and test splits below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_summary = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"train_min\": y_train.min(),\n",
    "        \"train_max\": y_train.max(),\n",
    "        \"test_min\": y_test.min(),\n",
    "        \"test_max\": y_test.max(),\n",
    "        \"train_var\": y_train.var(),\n",
    "        \"test_var\": y_test.var(),\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T\n",
    "df_split_summary[\"var_diff\"] = (\n",
    "    (df_split_summary[\"test_var\"] - df_split_summary[\"train_var\"])\n",
    "    / df_split_summary[\"train_var\"]\n",
    ") * 100\n",
    "display(df_split_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The train and test splits have a similar maximum target value and variance.\n",
    "2. The minimum of the training set is approx. $4,000 lower than that for the test set. Without apriori knowledge of real (OOS) data, it may not possible to control this minimum value without adding some bias to the random splitting process. Given the size of the the dataset overall (nearly 3,000 observations overall), it is possible that the minimum value could change as the number of observations grows. For the current case, this splitting will be left as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparing-for-modeling-and-feature-selection\"></a>\n",
    "\n",
    "## 6. [Preparing for modeling and feature selection](#preparing-for-modeling-and-feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for choosing a model and set of features, we'll define helper functions to help re-run several steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_validator(\n",
    "    pipe, model_name, num_feats_list, X, y, cols, scoring_metric=\"r2\",\n",
    "):\n",
    "    \"\"\"Perform KFCV for a single model or pipeline\"\"\"\n",
    "    cv_score_scaler = -1 if type(scoring_metric) == \"function\" else 1\n",
    "    d = {}\n",
    "    cv_results = cross_validate(\n",
    "        estimator=pipe,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        cv=RepeatedKFold(n_splits=5, n_repeats=5, random_state=1000),\n",
    "        scoring=scoring_metric,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    # print(cv_results['train_score'], cv_results['test_score'])\n",
    "    d[\"CV Train\"] = cv_score_scaler * np.mean(cv_results[\"train_score\"])\n",
    "    d[\"CV Test\"] = cv_score_scaler * np.mean(cv_results[\"test_score\"])\n",
    "    df_scores = pd.DataFrame.from_dict(d, orient=\"index\").T\n",
    "    # display(df_scores)\n",
    "    model_obj = cv_results[\"estimator\"][0].named_steps[\"reg\"]\n",
    "    if hasattr(model_obj, \"coef_\"):\n",
    "        # print(model_obj)\n",
    "        model_coefficients = model_obj.coef_\n",
    "    else:\n",
    "        model_coefficients = np.array([np.nan] * len(list(X)))\n",
    "    df_coefs = get_model_coefs(\n",
    "        coefs_array=model_coefficients, model_name=model_name, selected_cols=list(X),\n",
    "    ).reset_index(drop=False)\n",
    "    df_scores[\"model\"] = model_name\n",
    "    # print(cv_results[\"estimator\"][0].named_steps[\"reg\"].coef_)\n",
    "    # display(cv_results[\"estimator\"])\n",
    "    return (\n",
    "        df_scores,\n",
    "        df_coefs,\n",
    "        cv_results[\"estimator\"][0].named_steps[\"reg\"],\n",
    "        cv_results,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configuration_assesser(X, y, preprocessor, selected_cols, scoring_metric=\"r2\"):\n",
    "    \"\"\"\n",
    "    Perform KFCV on model(s) and return mean CV scores and model coefficients\n",
    "    \"\"\"\n",
    "    df_scores = []\n",
    "    df_coefs = []\n",
    "    fitted_models = []\n",
    "    cv_results_all = []\n",
    "    for model in [\n",
    "        linear_model.LinearRegression(),\n",
    "        linear_model.LassoCV(alphas=[-100.0, 10, 50.0, 1500.0], cv=5, max_iter=1500),\n",
    "        linear_model.RidgeCV(alphas=[-100.0, 10, 50.0, 1500.0], cv=5),\n",
    "        linear_model.ElasticNetCV(\n",
    "            l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
    "            alphas=[-100.0, 10, 50.0, 1500.0],\n",
    "            cv=5,\n",
    "        ),\n",
    "        DummyRegressor(strategy=\"mean\"),\n",
    "        DummyRegressor(strategy=\"median\"),\n",
    "    ]:\n",
    "        pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", model)])\n",
    "        model_name = (\n",
    "            type(model).__name__\n",
    "            if \"Dummy\" not in type(model).__name__\n",
    "            else type(model).__name__ + f\"_{model.strategy}\"\n",
    "        )\n",
    "        df_cv_scores, df_coef_vals, model_fitted, cv_results = model_cross_validator(\n",
    "            pipe=pipe,\n",
    "            model_name=model_name,\n",
    "            num_feats_list=nums,\n",
    "            X=X,\n",
    "            y=y,\n",
    "            cols=selected_cols,\n",
    "            scoring_metric=scoring_metric,\n",
    "        )\n",
    "        df_scores.append(df_cv_scores)\n",
    "        df_coefs.append(df_coef_vals)\n",
    "        fitted_models.append(model_fitted)\n",
    "        cv_results_all.append(cv_results)\n",
    "    df_sc = pd.concat(df_scores, axis=0).reset_index(drop=True)\n",
    "    df_coefs_all = reduce(lambda x, y: pd.merge(x, y, on=\"index\"), df_coefs)\n",
    "    return df_sc, df_coefs_all, fitted_models, cv_results_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to pass several models into this pipeline and perform feature selection as well as select a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"base-model-numerical-features-only\"></a>\n",
    "\n",
    "## 7. [Base model - numerical features only](#base-model-numerical-features-only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a base model, we'll consider only the numerical features in the listings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = nums\n",
    "display(X_train[selected_columns].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"raw-features\"></a>\n",
    "\n",
    "### 7.1. [Raw features](#raw-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll instantiate a pipeline to process (scale) the numerical features and perform no processing on the categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by only using the numerical feature that showed the strongest linear relationship to the target during visual exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(steps=[(\"scaler\", StandardScaler()),])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, [\"MPG\"])], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run this feature through the helper function `configuration_assesser` to perform K-fold cross-validation. We'll check performance for three linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[[\"MPG\"]].copy(),\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when `MPG` was replaced by `tank_volume`, the results found were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 6481.154313256067,\n",
    "            1: 6481.1620281921605,\n",
    "            2: 6481.185298013487,\n",
    "            3: 6481.1620281921605,\n",
    "            4: 7598.918657028725,\n",
    "            5: 7603.077576888589,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 6484.1624923519885,\n",
    "            1: 6484.1675242215315,\n",
    "            2: 6484.183501694554,\n",
    "            3: 6484.1675242215315,\n",
    "            4: 7600.869861593557,\n",
    "            5: 7606.112011843166,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `RMSE`s, for all models, were higher than with `MPG` alone, and due to the correlation between `MPG` and `tank_volume` (approx. -0.68 `R^2`), which would preclude interpretability of linear model coefficients, further analysis in this notebook will only use `MPG` and leaves out `tank_volume`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use all numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(steps=[(\"scaler\", StandardScaler()),])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, nums)], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[selected_columns].copy(),\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The low scores across in- and out-of-sample splits suggests we should add complexity by including more features. Since we saw some of the numerical features did not show perfectly normal distributions, this could influenced by the distribution of the input features - transformations to improve the distribution of these numerical features will be investigated next.\n",
    "2. Train and test scores are similar to eachother so, at the very least, the models are not overfitting to the training data without being able to generalize to OOS data.\n",
    "3. As mentioned in the `README.md` and above, the objective of building this model is to predict car prices to within \\$5,000 of the true price\n",
    "   - if we consider RMSE as our metric then, while this model is doing better than baseline models (taking the median or mean values), the desired level of performance is missed by approximately \\$1,000\n",
    "4. There is evidence of high bias (lacking complexity in features or type of model used) and low variance (difference between training and testing scores is small).\n",
    "5. Comparing the scores, for all models, relative to the single `MPG` feature (which earlier showed the strongest linear relationship to the target), we can see that the additional numerical features do offer a small benefit in terms of improving the cross-validation scores. For this reason, we'll retain those features in subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transformed-features\"></a>\n",
    "\n",
    "### 7.2. [Transformed features](#transformed-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on from the numerical features, we'll investigate the influence of the following feature transformations\n",
    "- Quantile ([1](https://www.hydrol-earth-syst-sci.net/16/1085/2012/), [2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2921808/))\n",
    "- Power (Yeo-Johnson) ([1](https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation), [2](https://www.stat.umn.edu/arc/yjpower.pdf))\n",
    "- `log(1 + x)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quantile-transformation\"></a>\n",
    "\n",
    "#### 7.2.1. [Quantile transformation](#quantile-transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll apply a quantile transformation to the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a quantile transformer object below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\n",
    "            \"quantiles\",\n",
    "            QuantileTransformer(n_quantiles=len(X_train), output_distribution=\"normal\"),\n",
    "        ),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, nums)], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll transform the features and the target, using this transformer object, and generate a pair plot showing the relationship between the\n",
    "- (top row) transformed features and the raw target\n",
    "- (bottom row) transformed features and the transformed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = QuantileTransformer(output_distribution=\"normal\")\n",
    "y_trans = tr.fit_transform(y_train.to_frame())\n",
    "cols = list(set(nums) - set([\"MPG\"]))\n",
    "if type(tr).__name__ != \"FunctionTransformer\":\n",
    "    y_trans = pd.Series(y_trans.flatten(), index=y_train.index, name=y_train.name)\n",
    "\n",
    "X_train_trans = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            Pipeline(\n",
    "                steps=[(\"quantiles\", numericals_transformer.named_steps[\"quantiles\"])]\n",
    "            ).fit_transform(X_train[cols]),\n",
    "            index=X_train.index,\n",
    "        ),\n",
    "        X_train[\"MPG\"],\n",
    "        y_trans,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_train_trans.columns = cols + [\"MPG\"] + [\"price_trans\"]\n",
    "plot_pairplot(\n",
    "    df=pd.concat([X_train_trans, y_train], axis=1),\n",
    "    yvar=[\"price\", \"price_trans\"],\n",
    "    vars_to_plot=list(X_train_trans),\n",
    "    color_by_col=None,\n",
    "    plot_specs=None,\n",
    "    wspace=0.05,\n",
    "    hspace=0.05,\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_histograms(\n",
    "    X=X_train,\n",
    "    cols_to_plot=list(set(selected_columns) - set([\"MPG\"])),\n",
    "    X_trans=X_train_trans,\n",
    "    hspace=0.2,\n",
    "    wspace=0.2,\n",
    "    alpha=0.95,\n",
    "    fig_size=(24, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. (From the histograms), the distributions have improved but (from the two rows of scatter plots above the histograms) the features still do not display a linear trend with the raw or transformed version of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll assess the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[selected_columns],\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The train and validation scores are moving together, but are slightly lower than for the raw features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show the coefficients selected by the models, sorted by value of the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stylers = []\n",
    "for model_name in [\"LinearRegression\", \"LassoCV\", \"RidgeCV\"]:\n",
    "    df_cs = pd.DataFrame(\n",
    "        df_coefs.sort_values(by=model_name, ascending=False)[[\"index\", model_name]]\n",
    "    )\n",
    "    df_styler = df_cs.style.set_table_attributes(\"style='display:inline'\").set_caption(\n",
    "        model_name\n",
    "    )\n",
    "    df_stylers.append(df_styler)\n",
    "\n",
    "display_html(\n",
    "    df_stylers[0]._repr_html_()\n",
    "    + df_stylers[1]._repr_html_()\n",
    "    + df_stylers[2]._repr_html_(),\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot the cross-validation scores below. If coefficients vary significantly when changing the input dataset their robustness is not guaranteed, and they should probably be interpreted with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv_scores(cv_results_all[0], selected_columns, fig_size=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. From the reasonably constrained error bars, it is reassuring that none of the model's coefficients vary significantly when varying the input data, as was done during cross-validation.\n",
    "2. It is clear that the `MPG` related features are prominent.\n",
    "3. It appears that *cars.com* customer reviews are the next most important numerical factor to users of the site but this is approx. X5 weaker than the miles per gallon for the vehicle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"yeo-johnson-transformation\"></a>\n",
    "\n",
    "#### 7.2.2. [Yeo-Johnson transformation](#yeo-johnson-transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll apply a quantile transformation to the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a power (yeo-johnson) transformer object below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, nums)], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll transform the features and generate a pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = PowerTransformer(method=\"yeo-johnson\")\n",
    "y_trans = tr.fit_transform(y_train.to_frame())\n",
    "cols = list(set(nums) - set([\"MPG\"]))\n",
    "if type(tr).__name__ != \"FunctionTransformer\":\n",
    "    y_trans = pd.Series(y_trans.flatten(), index=y_train.index, name=y_train.name)\n",
    "\n",
    "X_train_trans = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            Pipeline(\n",
    "                steps=[(\"power\", numericals_transformer.named_steps[\"power\"])]\n",
    "            ).fit_transform(X_train[cols]),\n",
    "            index=X_train.index,\n",
    "        ),\n",
    "        X_train[\"MPG\"],\n",
    "        y_trans,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_train_trans.columns = cols + [\"MPG\"] + [\"price_trans\"]\n",
    "plot_pairplot(\n",
    "    df=pd.concat([X_train_trans, y_train], axis=1),\n",
    "    yvar=[\"price\", \"price_trans\"],\n",
    "    vars_to_plot=list(X_train_trans),\n",
    "    color_by_col=None,\n",
    "    plot_specs=None,\n",
    "    wspace=0.05,\n",
    "    hspace=0.05,\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. While the scales have changed, the relationship between each of the transformed features and the target column remains the same\n",
    "   - the same observation is true for their relationship with the transformed target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_histograms(\n",
    "    X=X_train,\n",
    "    cols_to_plot=list(set(selected_columns) - set([\"MPG\"])),\n",
    "    X_trans=X_train_trans,\n",
    "    hspace=0.2,\n",
    "    wspace=0.2,\n",
    "    alpha=0.95,\n",
    "    fig_size=(24, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Distributions have improved but not all closely resemble a normal distribution.\n",
    "2. The 95% c.i.'s are shown as shaded green regions, based on the assumption that the distribution is normal and peaks at a mean (central) value, but these are unreliable as the transformed (and raw) distributions are not normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll assess the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[selected_columns],\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The train and validation scores approximately the same as those for the raw features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As befoire, we'll show the coefficients selected by the models, sorted by value of the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stylers = []\n",
    "for model_name in [\"LinearRegression\", \"LassoCV\", \"RidgeCV\"]:\n",
    "    df_cs = pd.DataFrame(\n",
    "        df_coefs.sort_values(by=model_name, ascending=False)[[\"index\", model_name]]\n",
    "    )\n",
    "    df_styler = df_cs.style.set_table_attributes(\"style='display:inline'\").set_caption(\n",
    "        model_name\n",
    "    )\n",
    "    df_stylers.append(df_styler)\n",
    "\n",
    "display_html(\n",
    "    df_stylers[0]._repr_html_()\n",
    "    + df_stylers[1]._repr_html_()\n",
    "    + df_stylers[2]._repr_html_(),\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, `MPG` related features are prominent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv_scores(cv_results_all[0], selected_columns, fig_size=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log1plus-transformation\"></a>\n",
    "\n",
    "#### 7.2.3. [`log1plus` transformation](#log1plus-transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply a `log1plus` transformation to the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a `log1plus` function transformer object below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"log1p\", FunctionTransformer(np.log1p, inverse_func=np.expm1)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, nums)], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(set(nums) - set([\"MPG\"]))\n",
    "X_train_trans = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(\n",
    "            Pipeline(\n",
    "                steps=[(\"log1p\", numericals_transformer.named_steps[\"log1p\"])]\n",
    "            ).fit_transform(X_train[cols]),\n",
    "            index=X_train.index,\n",
    "        ),\n",
    "        X_train[\"MPG\"],\n",
    "        y_trans,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_train_trans.columns = cols + [\"MPG\"] + [\"price_trans\"]\n",
    "plot_pairplot(\n",
    "    df=pd.concat([X_train_trans, y_train], axis=1),\n",
    "    yvar=[\"price\", \"price_trans\"],\n",
    "    vars_to_plot=list(X_train_trans),\n",
    "    color_by_col=None,\n",
    "    plot_specs=None,\n",
    "    wspace=0.05,\n",
    "    hspace=0.05,\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_histograms(\n",
    "    X=X_train,\n",
    "    cols_to_plot=list(set(nums) - set([\"MPG\"])),\n",
    "    X_trans=X_train_trans,\n",
    "    hspace=0.2,\n",
    "    wspace=0.2,\n",
    "    alpha=0.95,\n",
    "    fig_size=(18, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. Distributions appear similar to the Yeo-Johnson transformed distributions.\n",
    "2. Again, shaded regions can be ignored since the distributions are not normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll assess the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[selected_columns],\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show the coefficients selected by the models, sorted by value of the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stylers = []\n",
    "for model_name in [\"LinearRegression\", \"LassoCV\", \"RidgeCV\"]:\n",
    "    df_cs = pd.DataFrame(\n",
    "        df_coefs.sort_values(by=model_name, ascending=False)[[\"index\", model_name]]\n",
    "    )\n",
    "    df_styler = df_cs.style.set_table_attributes(\"style='display:inline'\").set_caption(\n",
    "        model_name\n",
    "    )\n",
    "    df_stylers.append(df_styler)\n",
    "\n",
    "display_html(\n",
    "    df_stylers[0]._repr_html_()\n",
    "    + df_stylers[1]._repr_html_()\n",
    "    + df_stylers[2]._repr_html_(),\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cv_scores(cv_results_all[0], selected_columns, fig_size=(8, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The train and validation scores approximately the same as those for the raw features.\n",
    "2. As with all transformed and raw features, train and validation scores move together.\n",
    "3. Coefficients do not vary significantly as the training data is changed\n",
    "   - Again, the `MPG` related feature is prominent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. While the quantile transformation best improved the shape of the distributions of the numerical features, the best compromise between shape of distribution and scores came from using the Yeo-Johnson transformation. This should be used in subsequent analysis.\n",
    "2. For all three transformations applied to the target `price` variable, very similar cross-validation scores were returned compared to those on untransformed data for `LinearRegression` and `RidgeCV`. `LassoCV` and `ElasticNetCV` (programmed as `LassoCV`) returned (unrealistic) zero error scores for the transformed version of the target. As the bimodal effect was weak in the raw data, the untransformed target data will be used in futher analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"polynomial-and-interaction-terms\"></a>\n",
    "\n",
    "### 7.3. [Polynomial and Interaction terms](#polynomial-and-interaction-terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now explore whether non-linearities and inter-feature interactions can result in a linear relationship with the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a ppolynomial feature transformer object below, that will also generate feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_train_poly = pd.DataFrame(\n",
    "    poly.fit_transform(X_train[selected_columns]),\n",
    "    columns=poly.get_feature_names(),\n",
    "    index=X_train.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now programmatically assemble names of the newly generated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_interaction_feats_names(\n",
    "    df: pd.DataFrame, source_column_names: list\n",
    ") -> list:\n",
    "    d = {f\"x{k}\": num_col_name for k, num_col_name in enumerate(source_column_names)}\n",
    "    new_feature_names = []\n",
    "    for c in list(df):\n",
    "        # print(c)\n",
    "        if \" \" not in c:\n",
    "            new_s = c.replace(c.split(\"^\")[0], d[c.split(\"^\")[0]])\n",
    "            # print(c, c.split(\"^\")[0], new_s)\n",
    "        else:\n",
    "            new_s = c.replace(c.split(\" \")[0], d[c.split(\" \")[0]]).replace(\n",
    "                c.split(\" \")[1], d[c.split(\" \")[1]]\n",
    "            )\n",
    "            # print(c, c.split(\" \")[0], c.split(\" \")[1], new_s)\n",
    "        new_feature_names.append(new_s)\n",
    "    return new_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly.columns = get_polynomial_interaction_feats_names(X_train_poly, nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll instantiate a new preprocessor object where we'll specify the few feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[(\"scaler\", StandardScaler())]), list(X_train_poly))\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairplot(\n",
    "    df=pd.concat([X_train_poly, y_train], axis=1),\n",
    "    yvar=[\"price\"],\n",
    "    vars_to_plot=list(X_train_poly),\n",
    "    color_by_col=None,\n",
    "    plot_specs=None,\n",
    "    wspace=0.05,\n",
    "    hspace=0.15,\n",
    "    fig_size=(48, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. With the exception of `MPG^2`, the simple non-linearities and feature interactions generated do not introduce new features with a linear relationship to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further problem with this is that the new features are correlated to eachother and to some of the existing fetures, as shown from a correlation heatmap below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_corr_map(\n",
    "    df=pd.concat([X_train_poly, y_train], axis=1),\n",
    "    annot=False,\n",
    "    annot_fmt=None,\n",
    "    fig_size=(12, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "1. Due to the problem of [non-moderate multi-collinearity](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) introduced by adding polynomial and interaction terms, we'll ignore this type of feature engineering and exclude non-linear and interaction terms from the data. The disadvantage is that this will lower the predictive power of the scraped numerical features, since these complex combinations will not be included, but it will maintain model coefficient interpretability which is one of the benefits of linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (i):** multi-collinearity must not be observed between input variables/features.\n",
    "\n",
    "Due to this requirement, we cannot include pairs of features that are highly correlated (high `R^2`, showing up as deep red or green on this heatmap of correlation coefficients); instead, we have to choose one and drop the other feature in such pairs. This rules out the features generated due to interaction terms and ploynomials. Alternatively, we could have retained these and been forced to use `LassoCV`, which would have zeroed out multi-collinear features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"include-categorical-features\"></a>\n",
    "\n",
    "## 8. [Include categorical features](#include-categorical-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"best-categorical-features\"></a>\n",
    "\n",
    "### 8.1. [Best categorical features](#best-categorical-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll show the best set of categorical features found from iteratively adding one feature at a time and checking the cross-validation scores which are shown further down in [sub-section 8.2](#summary-of-adding-categorical-features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_columns = nums + dummified_cols_flat\n",
    "display(X_train[selected_columns].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll again run these through the helper function in order to report on multiple models simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numericals_transformer, nums)], remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sc, df_coefs, m_fitted, cv_results_all = configuration_assesser(\n",
    "    X=X_train[selected_columns].copy(),\n",
    "    y=y_train,\n",
    "    preprocessor=preprocessor,\n",
    "    selected_cols=selected_columns,\n",
    "    scoring_metric=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scoring_metric=\"r2\",\n",
    ")\n",
    "display(df_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. KFCV scores improved suggesting the categorical features were helpful.\n",
    "2. In- and out-of-sample scores are moving together so regularization is not the top priority.\n",
    "3. The bias has reduced as the training scores have improved relative to only using the numerical features.\n",
    "4. The variance is relatively similar as, again, the training and testing scores are moving together and there is a small difference between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show the model coefficients below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stylers = []\n",
    "df_coeffs = []\n",
    "for model_name in [\"LinearRegression\", \"LassoCV\", \"RidgeCV\", \"ElasticNetCV\"]:\n",
    "    df_cs = pd.DataFrame(\n",
    "        df_coefs.sort_values(by=model_name, ascending=False)[[\"index\", model_name]]\n",
    "    )\n",
    "    df_cs.rename(columns={\"index\": \"feature\"}, inplace=True)\n",
    "    df_styler = df_cs.style.set_table_attributes(\"style='display:inline'\").set_caption(\n",
    "        model_name\n",
    "    )\n",
    "    df_stylers.append(df_styler)\n",
    "    df_coeffs.append(df_cs)\n",
    "\n",
    "display_html(\n",
    "    df_stylers[0]._repr_html_()\n",
    "    + df_stylers[1]._repr_html_()\n",
    "    + df_stylers[2]._repr_html_()\n",
    "    + df_stylers[3]._repr_html_(),\n",
    "    raw=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also print the amount of penalization chosen by cross validation for each of the regularized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_alphas = {}\n",
    "for model_obj in m_fitted[1:]:\n",
    "    mname = type(model_obj).__name__\n",
    "    if \"Dummy\" not in mname:\n",
    "        d_alphas[mname] = model_obj.alpha_\n",
    "pd.DataFrame.from_dict(d_alphas, orient=\"index\").rename(columns={0: \"best_alpha\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The scale of coefficients for the `RidgeCV` and `LinearRegression` are similar to eachother. Since `LassoCV` uses a different norm than `RidgeCV`, and we enforced this same norm in `ElasticNetCV` (from the `l1_ratio` hyper-parameter), both of these models (`LassoCV` and `ElasticNetCV`) are similar to eachother. Unlike regularization with `Lasso`, `Ridge` does not zero-out features; instead the weight is shared between the two or more predictive variables. Some features zeroed out by `LassoCV` carry strong weight (of the zeroed out features) in the `RidgeCV` coefficients. For `LassoCV`, this zeroed weight is distributed among the other features, excluding `miles-per-gallon`.\n",
    "2. The models are not over-fitting to the training data. We reach this conclusion from looking at the mean train (`CV Train`) and validation (`CV Test`) split scores from cross-validation. The main use of regularization is to counter over-fitting, which is evidently not a problem here as the test scores are very similar to the training scores. With strong regularization (the best chosen `alpha` values for `RidgeCV`, `ElasticNetCV` and `LasoCV` was 10), the coefficients of all the regularized models lie in a similar min-max range. However, the range of coefficients is larger for `LinearRegression`, which could be caused by multi-collinearity between combinations of features ([1](https://stats.stackexchange.com/a/421511/144450), [2](https://stats.stackexchange.com/a/64218/144450)) especially as high-cardinality categorical features are being added. This suggests regularization might be beneficial to control these coefficient values from blowing up and ensure their interpretability later on. For this reason, based on scores with this best set of categorical features and the earlier chosen numerical ones, choosing a regularized model could be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary-of-adding-categorical-features\"></a>\n",
    "\n",
    "### 8.2. [Summary of adding categorical features](#summary-of-adding-categorical-features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of iteratively adding a single categorical feature, referred to [above](#best-categorical-features), to a base model comprising `Fuel Type`, `Drivetrain` and `State`, is reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `year` was added to the a base categorical feature set of `Fuel Type`, `Drivetrain` and `State`, the results found were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 5473.387724126905,\n",
    "            1: 5483.134595981816,\n",
    "            2: 5488.360808508998,\n",
    "            3: 5483.134595981816,\n",
    "            4: 7600.754921755837,\n",
    "            5: 7604.997140152696,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 5512.5436290325315,\n",
    "            1: 5512.969239744665,\n",
    "            2: 5518.157019603685,\n",
    "            3: 5512.969239744665,\n",
    "            4: 7601.565006474694,\n",
    "            5: 7605.959439060718,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `year` and `trans_speed` were added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 5102.3929433172125,\n",
    "            1: 5120.955154363325,\n",
    "            2: 5134.480227590843,\n",
    "            3: 5120.955154363325,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 5170.96850526448,\n",
    "            1: 5183.600486912249,\n",
    "            2: 5196.18544570807,\n",
    "            3: 5183.600486912249,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `year`, `trans_speed` and `seller_rating`, the following are the scores and the `LinearRegression` model returned on one-hot encoded `seller_rating` feature with a fitted coefficient that was approx. x3 larger than other features and very large `RMSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4800.603630049307,\n",
    "            1: 4847.639393495595,\n",
    "            2: 4854.909357773905,\n",
    "            3: 4847.639393495595,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 11925870424226.977,\n",
    "            1: 4948.559967924518,\n",
    "            2: 4954.095539079443,\n",
    "            3: 4948.559967924518,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar observations, of exploding validation set cross-validation scores and higher coefficients for a very small subset of features, were found when `consumer_stars` was added to `year` and `trans_speed`. The scores are shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4827.866967593768,\n",
    "            1: 4865.630116619575,\n",
    "            2: 4875.3716286546705,\n",
    "            3: 4865.630116619575,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 332152587630.0533,\n",
    "            1: 4959.278431865635,\n",
    "            2: 4964.551866565196,\n",
    "            3: 4959.278431865635,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `year`, `trans_speed` and `Comfort` were added, scores and coefficients were better behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4956.976841898949,\n",
    "            1: 4977.043027336144,\n",
    "            2: 4986.760101298544,\n",
    "            3: 4977.043027336144,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 5027.286938265067,\n",
    "            1: 5041.543322429582,\n",
    "            2: 5049.651705377978,\n",
    "            3: 5041.543322429582,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `year`, `trans_speed`, `Comfort` and `Performance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4953.945333192869,\n",
    "            1: 4975.043344549585,\n",
    "            2: 4983.96203318336,\n",
    "            3: 4975.043344549585,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 5039.172005801402,\n",
    "            1: 5043.034545328092,\n",
    "            2: 5053.17241479394,\n",
    "            3: 5043.034545328092,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `year`, `trans_speed`, `Comfort` and `Exterior Styling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4941.72801416282,\n",
    "            1: 4961.553376968924,\n",
    "            2: 4970.584250384024,\n",
    "            3: 4961.553376968924,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 5015.9265156533775,\n",
    "            1: 5029.440826583011,\n",
    "            2: 5036.944482024336,\n",
    "            3: 5029.440826583011,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `year`, `trans_speed`, `Comfort`, `Exterior Styling` and `Interior_Design`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4849.112335746027,\n",
    "            1: 4870.507769797931,\n",
    "            2: 4877.353421220105,\n",
    "            3: 4870.507769797931,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 4927.120360636142,\n",
    "            1: 4938.4591029994,\n",
    "            2: 4946.060803431385,\n",
    "            3: 4938.4591029994,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with `year`, `trans_speed`, `Comfort`, `Exterior Styling`, `Interior_Design` and `Reliability`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"CV Train\": {\n",
    "            0: 4829.946163513605,\n",
    "            1: 4869.310940006296,\n",
    "            2: 4872.917805000394,\n",
    "            3: 4869.310940006296,\n",
    "            4: 7598.728897140117,\n",
    "            5: 7602.975035670904,\n",
    "        },\n",
    "        \"CV Test\": {\n",
    "            0: 4914.357334726612,\n",
    "            1: 4940.607277684173,\n",
    "            2: 4947.6334368637135,\n",
    "            3: 4940.607277684173,\n",
    "            4: 7600.587716921647,\n",
    "            5: 7607.208725314137,\n",
    "        },\n",
    "        \"model\": {\n",
    "            0: \"LinearRegression\",\n",
    "            1: \"LassoCV\",\n",
    "            2: \"RidgeCV\",\n",
    "            3: \"ElasticNetCV\",\n",
    "            4: \"DummyRegressor_mean\",\n",
    "            5: \"DummyRegressor_median\",\n",
    "        },\n",
    "    },\n",
    "    orient=\"index\",\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. As the out-of-sample scores are not larger than the in-sample ones, there does not appear to be evidence of overfitting.\n",
    "2. Consistent improvement in `CV Test` (validation split) scores, across all models, for `year`, `trans_speed`, `Comfort`, `Exterior Styling` and `Interior_Design`\n",
    "2. When `Reliability` was added, although `LinearRegression` reported slightly lower scores, the other models were higher than when `Reliability` was not included, so this feature was not retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "As mentioned earlier, a regularized model will be chosen as all models are similar in scores, but coefficients seem slightly bettre behaved for those that were penalized. `RidgeCV` was selected, but subsequent sections showed similarity between results with `RidgeCV` and `LassoCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"assessing-best-model\"></a>\n",
    "\n",
    "## 9. [Assessing best model](#assessing-best-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll assess performane of the best regressor, which we decided to be `RidgeCV()` based on the cross-validation runs and the resulting range of model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"extracting-best-model\"></a>\n",
    "\n",
    "### 9.1. [Extracting best model](#extracting-best-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll instantiate a new model object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model is extracted from the list of returned fitted models from cross-validation, but this model will be re-fit on all the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_row = 2  # 2 since RidgeCV was third among the list of models tried"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = df_sc.loc[model_row, \"model\"]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll instantiate a pipeline use the newly instantiated best model object to it, after the Yeo-Johnson feature transformer preprocessing step that was determined from cross-validation earlier to be the best for transforming the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"reg\", m_fitted[model_row])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline object will be used in subsequent model evaluation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"get-best-model-coefficients\"></a>\n",
    "\n",
    "### 9.2. [Get best model coefficients](#get-best-model-coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll visualize the feature coefficients for the best model. Since this is a linear model, we can directly interpret the model'ls coefficients because we have\n",
    "- scaled features\n",
    "- removed multi-collinear features\n",
    "- not observed wildly varying coefficient values during cross-validation earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_best = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        dict(zip(selected_columns, pipe.named_steps[\"reg\"].coef_.tolist())),\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"feature\", 0: best_model})\n",
    "    .sort_values(by=best_model, ascending=False)\n",
    ")\n",
    "plot_coef_plot(xvar=best_model, df=df_viz_best, fig_size=(12, 12), save_fig=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The dominant features are related to `miles-per-gallon`, which is a numerical feature, and this is negative since its relationship to the target (listing price) was seen earlier to be negative.\n",
    "   - other numerical features remain several times smaller than this feature\n",
    "2. Dominant categorical features are `Transmission Speed`, `Drivetrain` and `fuelType` and they are stronger predictors than the other numerical features used.\n",
    "3. It is notable that that `State` is not one of the strongest predictors. So, if it is included, it won't drive predictions significantly - if it was a significant predictor, we have been better off using a separate model per state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"residuals-for-predictions\"></a>\n",
    "\n",
    "### 9.3. [Residuals for predictions](#residuals-for-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the residual on the test set, using this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comp, pipe_final_fitted, fit_residual = plot_residual_manually(\n",
    "    estimator=pipe,\n",
    "    X_train=X_train[selected_columns],\n",
    "    y_train=y_train,\n",
    "    X_test=X_test[selected_columns],\n",
    "    y_test=y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. A pattern is not clearly visible in the residuals. However, it does not resemble white noise due to the upper bound on vehicle prices that was set when retrieving the listings. This appears here as a 45-degree line, intersecting the x-axis at y=0 and at the x-value corresponding to the highest listed price in the test data.\n",
    "2. A clear fan stricture increasing from `x=0` is not visible.\n",
    "3. Since there is minor evidence of pattern in the plot, so some [heteroskedasticity (requires a distinctive fan or cone shape in residual plots)](https://statisticsbyjim.com/regression/heteroscedasticity-regression/) is likely present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (i):** a linear **additive** relationship exists between the dependent and each independent variable.\n",
    "\n",
    "and\n",
    "\n",
    "**Duke (iv):** model residuals should display a normal distributed, centered around zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are centered at a value of 0 as expected (the distribution will give a complimentary assessment of this later) and we don't see a pattern which would indicate systematic non-additive errors in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (iii)**: errors terms show constant variance (no homoskedasticity)\n",
    "\n",
    "As mentioned above, some qualitative evidence of heteroskedasticity is present due to the enforced upper bound of \\$45,000 for listing price. The rule of thumb is OLS regression isn't too impacted by heteroscedasticity as long as the maximum variance is not greater than four times the minimum variance.\n",
    "\n",
    "Here, due to the lower part of the residual plot being affected by the upper-bound cutoff of \\$45,000, only the top half will be considered\n",
    "- starting at the `Predicted Value` of \\$20,000, the top half min-max range is approx. 5,000 USD\n",
    "  - this is also true for the bottom half-range\n",
    "- at the average (central) `Predicted Value` of \\$30,000, the half-range is approx. 12,000 USD\n",
    "  - this is also true for the bottom half-range\n",
    "- ending at the `Predicted Value` of \\$40,000, the upper half-range is approx. 14,000 USD\n",
    "  - this is cannot be determined for the bottom half-range\n",
    "\n",
    "As the min-max range is approx. 3, the best model here is not in violation of this rule of thumb. Also a large number of the residuals are less than these extreme values. These appear to suggest minimal heteroskedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke ii**: Errors are statistically independent, meaning there is no correlation between consecutive errors\n",
    "\n",
    "This cannot be tested here since the listing date was not scraped and this dataset is not a timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"residual-distributions\"></a>\n",
    "\n",
    "#### 9.3.1. [Residual distributions](#residual-distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at a histogram of the train and test splits, as well as their residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe_final_fitted.predict(X_test[selected_columns])\n",
    "y_pred_train = pipe_final_fitted.predict(X_train[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_histograms(\n",
    "    X=pd.concat(\n",
    "        [\n",
    "            y_train.reset_index(drop=True).rename(\"train\"),\n",
    "            y_test.reset_index(drop=True).rename(\"test\"),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    cols_to_plot=[\"train\", \"test\"],\n",
    "    hspace=0.2,\n",
    "    wspace=0.2,\n",
    "    fig_size=(12, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_histograms(\n",
    "    X=pd.concat(\n",
    "        [\n",
    "            (y_pred_train - y_train).rename(\"train residual\").to_frame(),\n",
    "            (y_pred - y_test).rename(\"test residual\").to_frame(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    ),\n",
    "    cols_to_plot=[\"train residual\", \"test residual\"],\n",
    "    hspace=0.2,\n",
    "    wspace=0.2,\n",
    "    fig_size=(12, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. As expected, the distributions have a right upper bound due to the maximum value enforced during retrieval of the listings.\n",
    "2. Both distributions appear to be broadened Gaussian shapes, with a drop in the center around the average retrieved car price of $30,000.\n",
    "3. The residuals are closer to normal distributions with a clear single peak, though both are also broadened in width shown by the lower-than-expected y-value where the green shaded 95 per-cent c.i. intersects the disitibution. Both are centered at zero, as expected.\n",
    "\n",
    "These suggest if additional data were acquired at around the average value, the shape of the input data could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (iv): (contd)** model residuals should display a normal distributed, centered around zero\n",
    "\n",
    "From the distributions, the residuals are centreed at zero but show an overall broadened normal distribution suggesting there are larger than ideal positive and negative residuals. The distribution of the test set is not perfectly symmetric; residuals of approx USD 4,000-5,000 and moreso USD6,000-8,000 are higher than expected for a normal distribution - the latter appears to be driving the skew more strongly however the former is of concern for the current project since it falls inside the error tolerance specified for predicting vehicle prices namely \\$5,000. These sources of deviations from the normal shape of the distribution could intriduce bias into the model's predictions and we'll explore this further later in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"worst-performing-predictions\"></a>\n",
    "\n",
    "#### 9.3.2. [Worst performing predictions](#worst-performing-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll show the top 10 highest residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_comp[\"pct_res\"] = (X_comp[\"res\"] / X_comp[\"test\"]) * 100\n",
    "display(\n",
    "    X_comp.sort_values(by=\"res\", ascending=False)[\n",
    "        [\"State=WA\", \"res\", \"test\", \"pct_res\"]\n",
    "    ].nlargest(20, [\"res\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Percent of predictions different from true price by more than $5,000: \"\n",
    "    f\"{(len(X_comp.loc[X_comp['res'] > 5000]) / len(X_comp)) * 100:.1f}% \"\n",
    "    f\"(out of {len(X_comp)} obs.)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. We can see that the largest residual (corresponding to the worst prediction) is approximately 12,500 dollars on a car whose price is approximately 27,000 dollars for a percent residual of approx. 46%.\n",
    "2. The model's top 6 worst predictions contain five from the state of Washington, and all of these miss the true `price` by more than \\$10,000.\n",
    "4. Approximately thirteen percent of the predictions are different from the true prices by more than \\$5,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"residuals-separated-by-state\"></a>\n",
    "\n",
    "#### 9.3.3. [Residuals separated by State](#residuals-separated-by-state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll check if the residuals show a trend based on the `State`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.scatterplot(x=\"pred\", y=\"res\", hue=\"State=WA\", data=X_comp, ax=ax, s=80)\n",
    "ax.set_xlabel(\"Predicted Value\")\n",
    "ax.set_ylabel(\"Residuals\")\n",
    "ax.set_title(\"Model Residuals, by State\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. As there aren't any clear patterns by `State`, this suggests using a single model, with `State` as a categorical feature, is acceptable for this data set. Creating a separate model for each state is likely not the main contributor to deviation in model predictions from the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quantile-quantile-plot\"></a>\n",
    "\n",
    "#### 9.3.4. [Quantile-quantile plot](#quantile-quantile-plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot a QQ plot of the residual in order to check that the regression is linear in model parameters and correctly specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_qq(fit_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (i) (contd)**: a linear additive relationship exists between the dependent and each independent variable.\n",
    "\n",
    "We can check this by inspecting the QQ plot and require points in the central section to be very close to the 45-degree line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-Q (quantile-quantile) plot helps qualitatively determine if it is plausible that the data (residuals) come from a normal distribution. We can use the Normal Q-Q plot to check the assumption that the target data comes from a normal distribution. This plot is produced by graphing both sets of quantiles against one another. If both sets of quantiles have came from the same distribution, points should form a straight line. ([Link](https://data.library.virginia.edu/understanding-q-q-plots/)) \n",
    "\n",
    "The middle section of the QQ plot shown is very close to the diagonal red line, with evidence of light tailing (due to curvature at lower and higher quantiles). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"permutation-importances\"></a>\n",
    "\n",
    "### 9.4. [Permutation Importances](#permutation-importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at the permutation importance of all the features that we've chosen. We'll use the version of the categorical data that has not been one-hot encoded and include the one-hot encoding step in a pipeline. This way the original categorical feature names, and not the dummified versions, will appear in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll manually apply the filtering needed - previously this was done inside a transformer pipeline that could also operate on target values. Now we'll do this separately on the train and test split since the pipeline will need to include a regression model at the end and so can't opreate on the target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = X.loc[list(set(X.index.tolist()) - set(train_indices))].index\n",
    "selected_columns_temp = nums + cats\n",
    "X_train_filtered = X.loc[train_indices][\n",
    "    (X.loc[train_indices][\"MPG\"] < 80) & (X.loc[train_indices][\"Mileage\"] < 10000)\n",
    "][selected_columns_temp].dropna(subset=selected_columns_temp, how=\"any\")\n",
    "y_train_filtered = y.loc[X_train_filtered.index]\n",
    "X_test_filtered = X.loc[test_indices][\n",
    "    (X.loc[test_indices][\"MPG\"] < 80) & (X.loc[test_indices][\"Mileage\"] < 10000)\n",
    "][selected_columns_temp].dropna(subset=selected_columns_temp, how=\"any\")\n",
    "y_test_filtered = y.loc[X_test_filtered.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the filtering steps from the transformer pipeline and add the preprocessing and classification steps in - this will be the new pipeline that includes the regression model and can be used as an input into the `permutation_importances` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # (\"filter_mpg\", DFFilterNumerical(\"MPG\", 80, \"lt\")),\n",
    "        # (\"filter_mileage\", DFFilterNumerical(\"Mileage\", 10000, \"lt\")),\n",
    "        (\"nan\", DFDropNa(cols=all_selected, how=\"any\")),\n",
    "        (\"ohe\", DFOneHotEncoder(cats, \"=\")),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"reg\", m_fitted[model_row]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the pipeline on training data and predict + score on OOS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_transformer.fit(X_train_filtered, y_train_filtered)\n",
    "y_test_pred = pipe_transformer.predict(X_test_filtered)\n",
    "y_train_pred = pipe_transformer.predict(X_train_filtered)\n",
    "display(\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"train\": r2_score(y_train_filtered, y_train_pred),\n",
    "            \"test\": r2_score(y_test_filtered, y_test_pred),\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"r2\", \"index\": \"split\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained this pipeline and made predictions on the testing data, we'll now get the linear model (`RidgeCV`) coefficients and permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_viz_best = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        dict(\n",
    "            zip(\n",
    "                selected_columns_temp,\n",
    "                pipe_transformer.named_steps[\"reg\"].coef_.tolist(),\n",
    "            )\n",
    "        ),\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"feature\", 0: best_model})\n",
    ")\n",
    "df_viz_best = df_viz_best.reindex(\n",
    "    df_viz_best[best_model].abs().sort_values(ascending=False).index\n",
    ")\n",
    "r = permutation_importance(\n",
    "    pipe_transformer,\n",
    "    X_train_filtered,\n",
    "    y_train_filtered,\n",
    "    scoring=make_scorer(rmse_scorer, greater_is_better=True),\n",
    "    # scoring=\"r2\",\n",
    "    n_repeats=30,\n",
    "    random_state=0,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "sorted_idx = r.importances_mean.argsort()\n",
    "df_perm_imp = pd.DataFrame(r.importances[sorted_idx].T)\n",
    "df_perm_imp.columns = X_test_filtered.columns[sorted_idx]\n",
    "df_perm_imp = df_perm_imp.unstack(level=-1).reset_index()\n",
    "df_perm_imp = df_perm_imp.rename(\n",
    "    columns={\"level_0\": \"feature\", \"level_1\": \"trial\", 0: \"importance\"}\n",
    ")\n",
    "df_perm_imp = (\n",
    "    df_perm_imp.set_index(\"feature\")\n",
    "    .loc[X_test_filtered.columns[sorted_idx][::-1]]\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing the linear model coefficients and permutation importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_side_by_side_bar_box(\n",
    "    df_viz_best,\n",
    "    best_model,\n",
    "    df_perm_imp,\n",
    "    X_train_filtered.columns[sorted_idx][::-1].tolist(),\n",
    "    \"darkred\",\n",
    "    \"blue\",\n",
    "    25,\n",
    "    \"blue\",\n",
    "    12,\n",
    "    12,\n",
    "    115,\n",
    "    -3,\n",
    "    (350, 200),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The error bars show that the feature importances do not vary wildly as they are randomized so the influence of each feature on its own is reasonably well constrained with minimal influence of outliers (points lying outside the error bar range).\n",
    "2. As with the linear model coefficients of the best model, *Miles-Per-Gallon* is the most important factor followed by *Drivetrain*. Permuting (randomizing) their respective orders (while holding all other factors constant) has the largest effect on the model's predictions of the OOS data, of all the factors that were investigated here.\n",
    "3. Based on permutation importances, there are some features whose importance is relatively much weaker than the absolute values of their model coefficients suggest - `Comfort` and `year`. The opposite is true for `trans_speed`, which ranks higher in permutation importance than its coefficient suggests. Since permutation importance is free of problems in interpreting regularized model coefficients and correlated variables, it could be a better choice for the relative importance of these three features to the model. With the exception of these three features, there is general agreement between the relative positioning of the features on each of this plots, which is reassuring since it suggests errors in interpreting model coefficients is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prediction-error\"></a>\n",
    "\n",
    "### 9.5. [Prediction error](#prediction-error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the prediction error plot to show the actual targets from the dataset against the predicted values generated by our model. This will allow for a visualization of how much variance is in the model. We can use this diagnostic plot by comparing it against the 45 degree (slope of 1) line which corresponds to the prediction and model exactly matching eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_yb_prediction_error(\n",
    "    estimator=pipe_final_fitted, X=X_test[selected_columns], y=y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Duke (i): (contd.)** a linear additive relationship exists between the dependent and each independent variable.\n",
    "\n",
    "We can check this from a plot of observed data vs predicted values for the test set - the predicted values should be symmetric around their line of best fit. While this is quanitatively the case from visual inspection and quantitatively from the `R^2` of 0.61, it is also clear the there is a difference between the best-possible linear fit and the one found here.i.e. there is room for improvement in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. The true points (grey dashed line) are asymetric relative to the prediction best fit line (black dashed line), attributed to the sub-optimal `R^2` value from the fitted model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary-of-scoring-metrics\"></a>\n",
    "\n",
    "### 9.6. [Summary of scoring metrics](#summary-of-scoring-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll print scoring metrics on predictions made using the final pipeline (estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are shown first for the test set only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = get_test_metrics(\n",
    "    metrics_wanted=[\"MAE\", \"MDAE\", \"RMSE\", \"MAXE\", \"MAPE\", \"R2\", \".score()\"],\n",
    "    X_r=X_test[selected_columns],\n",
    "    r=y_test,\n",
    "    f=y_pred,\n",
    "    est=pipe_final_fitted,\n",
    ")\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a subset of metrics, in addition to max and min values, for the training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "rmse_test = mean_squared_error(y_test, y_pred, squared=False)\n",
    "df_summ = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"r2_train\": r2_train,\n",
    "            \"r2_test\": r2_test,\n",
    "            \"rmse_train\": rmse_train,\n",
    "            \"rmse_test\": rmse_test,\n",
    "            \"min_train\": y_train.min(),\n",
    "            \"max_train\": y_train.max(),\n",
    "            \"min_test\": y_test.min(),\n",
    "            \"max_test\": y_test.max(),\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .rename(columns={0: \"value\"})\n",
    "    .astype(float)\n",
    "    .reset_index()\n",
    ")\n",
    "df_summ[[\"metric\", \"split\"]] = df_summ[\"index\"].str.split(\"_\", expand=True)\n",
    "df_summ[[\"value\", \"metric\", \"split\"]].pivot(\n",
    "    index=\"split\", columns=\"metric\", values=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. As seen earlier, the training set has a lower minimum value than the testing set.\n",
    "2. Both `R^2` and RMSE are better for the testing data than for the training data. A possible reason is inadequate splitting of the training and testing data.i.e. randomly setting 20% aside as the testing split may not be appropriate based on the nature of the dataset. Multiple train-test splits could be used to further explore this option and this will be done below.\n",
    "3. Current model performance on the test set is just below (better than) the desired level for this project.i.e. the desired RMSE is \\$5,000, which is just above the best model's preformance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison-of-ordinary-linear-regression-between-sklearn-and-statsmodels\"></a>\n",
    "\n",
    "#### 9.6.1. [Comparison of ordinary linear regression between `sklearn` and `statsmodels`](#comparison-of-ordinary-linear-regression-between-sklearn-and-statsmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a brief comparison between non-penalized OLS regression using the `statsmodels` library and `LinearRegression()` from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above filtered training and testing splits that were used in the [Permutation Importances](#permutation-importances) section, and pass this through the same transformer pipeline instantiated in that section, without the `sklearn` regressor object at the end, to transform the filtered training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"nan\", DFDropNa(cols=all_selected, how=\"any\")),\n",
    "        (\"ohe\", DFOneHotEncoder(cats, \"=\")),\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    ]\n",
    ")\n",
    "X_train_filtered_transformed = pd.DataFrame(\n",
    "    pipe_transformer.fit_transform(X_train_filtered, y_train_filtered),\n",
    "    columns=pipe_transformer.named_steps[\"ohe\"].get_feature_names(),\n",
    "    index=X_train_filtered.index,\n",
    ")\n",
    "X_test_filtered_transformed = pd.DataFrame(\n",
    "    pipe_transformer.transform(X_test_filtered),\n",
    "    columns=pipe_transformer.named_steps[\"ohe\"].get_feature_names(),\n",
    "    index=X_test_filtered.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-statsmodels\"></a>\n",
    "\n",
    "##### **9.6.1.1. [Using `statsmodels`](#using-statsmodels)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS model is trained on the transformed training data and predictions are made ([1](https://stackoverflow.com/a/57816992/4057186), [2](https://www.statsmodels.org/stable/examples/notebooks/generated/predict.html)) on the transformed training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olsres = sm.OLS(y_train_filtered, X_train_filtered_transformed).fit()\n",
    "y_pred_train_sm = olsres.predict(X_train_filtered_transformed)\n",
    "y_pred_test_sm = olsres.predict(X_test_filtered_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting scoring metrics for predictions made using `statsmodels` OLS API are shown below, for train and test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_sm = r2_score(y_train_filtered, y_pred_train_sm)\n",
    "r2_test_sm = r2_score(y_test_filtered, y_pred_test_sm)\n",
    "rmse_train_sm = mean_squared_error(y_train_filtered, y_pred_train_sm, squared=False)\n",
    "rmse_test_sm = mean_squared_error(y_test_filtered, y_pred_test_sm, squared=False)\n",
    "df_summ_sm = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"r2_train\": r2_train_sm,\n",
    "            \"r2_test\": r2_test_sm,\n",
    "            \"rmse_train\": rmse_train_sm,\n",
    "            \"rmse_test\": rmse_test_sm,\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .rename(columns={0: \"value\"})\n",
    "    .astype(float)\n",
    "    .reset_index()\n",
    ")\n",
    "df_summ_sm[[\"metric\", \"split\"]] = df_summ_sm[\"index\"].str.split(\"_\", expand=True)\n",
    "df_summ_sm[[\"value\", \"metric\", \"split\"]].pivot(\n",
    "    index=\"split\", columns=\"metric\", values=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-sklearn\"></a>\n",
    "\n",
    "##### **9.6.1.1. [Using `sklearn`](#using-sklearn)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparison, using `sklearn`'s `LinearRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train_filtered_transformed, y_train_filtered)\n",
    "y_pred_train_sk = lm.predict(X_train_filtered_transformed)\n",
    "y_pred_test_sk = lm.predict(X_test_filtered_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train_sk = r2_score(y_train_filtered, y_pred_train_sk)\n",
    "r2_test_sk = r2_score(y_test_filtered, y_pred_test_sk)\n",
    "rmse_train_sk = mean_squared_error(y_train_filtered, y_pred_train_sk, squared=False)\n",
    "rmse_test_sk = mean_squared_error(y_test_filtered, y_pred_test_sk, squared=False)\n",
    "df_summ_sk = (\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"r2_train\": r2_train_sk,\n",
    "            \"r2_test\": r2_test_sk,\n",
    "            \"rmse_train\": rmse_train_sk,\n",
    "            \"rmse_test\": rmse_test_sk,\n",
    "        },\n",
    "        orient=\"index\",\n",
    "    )\n",
    "    .rename(columns={0: \"value\"})\n",
    "    .astype(float)\n",
    "    .reset_index()\n",
    ")\n",
    "df_summ_sk[[\"metric\", \"split\"]] = df_summ_sk[\"index\"].str.split(\"_\", expand=True)\n",
    "df_summ_sk[[\"value\", \"metric\", \"split\"]].pivot(\n",
    "    index=\"split\", columns=\"metric\", values=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are very similar, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"check-influence-of-choice-of-test-split\"></a>\n",
    "\n",
    "### 9.8. [Check influence of choice of test split](#check-influence-of-choice-of-test-split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now explore how the choice of splitting out the test set influences the model's performance. We'll do this using nested k-fold cross-validation on all (training + testing) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do this with two plots\n",
    "- (top) **fixed** train+validation and test sets in the outer cross-validation splits, with variable inner splits\n",
    "  - this will be equivalent to using the test set from the train-test split that was created above\n",
    "  - we would expect the train and test scores here to match those found from the best model and summarized in the table above\n",
    "- (bottom) **varible** train+validation and test sets in outer cross-validation splits, with variable inner splits\n",
    "  - here, the test split will be varied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nested_cv(\n",
    "    pipe_final_fitted,\n",
    "    pd.concat([X_train, X_test])[selected_columns],\n",
    "    pd.concat([y_train, y_test]),\n",
    "    NUM_NESTED_CV_TRIALS,\n",
    "    PredefinedSplit(([-1] * len(y_train)) + [0] * len(y_test)),\n",
    "    scorer=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    # scorer=\"r2\",\n",
    "    outer_cv_n_splits=5,\n",
    "    inner_cv_n_splits=5,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    "    fig_size=(12, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "1. Hyper-parameter optimization is not performed in the inner cross-validation; instead, the best found model is used as-is.\n",
    "2. Average scores are shown as a solid line, with the shaded region showing the range of scores.\n",
    "3. Due to the random nature of generating the test splits, the individual test split generated in the top plot is not guaranteed to re-occur in the bottom plot. For this reason, the exact test score that is returned when the test split is fixed does not appear when the test set is variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "1. As expected, the top plot shows train and test scores (where the train - the train+validation set - and test set are always fixed, to match the single test-set splitting used earlier) that match those found above; as we can see, the training score is worse than the testing score.\n",
    "2. From the bottom plot, depending on the choice of test split, the test score can be larger than the train score for this dataset. Across multiple test splits, this is not the case on average and the mean testing score is slightly worse than the mean training score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "We have to think of test set generalization error as a random variable. The mean of the test distribution is the true generalization error for the test set. A distribution can be characterized by a mean and also a standard deviation. What makes one get a different test set sample is the draw of the data randomly returned.\n",
    "\n",
    "We take samples from this distribution curve. Depending on the sample taken, it may or may not a good estimate of the mean of the distribution. We may get a good estimate of the generalization error. However, that is not always the case and the sample may be a poor estimate of the mean of the distribution. When a model is deployed to production, the sampled value for this error may also be significantly different from the mean just by chance.\n",
    "\n",
    "Normal cross-validation using a train+validation split and test split, but does not account for the variance of the test set's error distribution due to the fact that we're taking a random test split. So, instead of using one test split, we could use several test splits to account for this variance. If we take multiple samples from the Gaussian, we would get a better estimate of the mean of the generalization error (which is the error of the test set).\n",
    "\n",
    "For five test splits, we split the entire dataset into five possible test splits rather than just one. Now, each sample participates in the test split exactly once. We fit the same model five times and we get five test set errors, giving a mean test error and minimum and maximum of the test error. This (max - min) allows us to explore a range of OOS scores. It may be possible that one of these five test set scores is larger than the train+validatoin set score, but that may not be true for the other four test sets resulting in an average test set score than is infact lower than the train set score. This appears to be the case here, as seen from the solid red line (average test score) that is lower than the training+validation set score, even though the maximum test set score is higher than than the maximum train set score.\n",
    "\n",
    "Small datasets, such as this one, are particularly susceptible to this problem. In this case, it is likely that the small size of the test set (approx. 500 observations) produces this large variability, which is reduced in the much larger (approx. X5) train+validation set where the influence of a small number of observations driving the increased test set variability is reduced. A better estimate of model performance in this case may be given by the averaged test set score (+/- its standard deviation) from nested cross-validation than a single test set score. Additionally, another round of filtering may be warranted to remove the influence of observations that are driving up the test set error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"check-of-bias-and-variance\"></a>\n",
    "\n",
    "### 9.8. [Check of bias and variance](#check-of-bias-and-variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll explore bias and variance related to this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"reg\", RandomForestRegressor(max_leaf_nodes=350)),\n",
    "    ]\n",
    ")\n",
    "show_yb_learning_curves(\n",
    "    X=pd.concat([X_train[selected_columns], X_test[selected_columns]]),\n",
    "    y=pd.concat([y_train, y_test]),\n",
    "    pipes=[pipe, prf],\n",
    "    hspace=0,\n",
    "    wspace=0.1,\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    scorer=make_scorer(rmse_scorer, greater_is_better=False),\n",
    "    fig_size=(16, 8),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations and Discussion**\n",
    "1. Both training and validation scores show a minor increase upto approx. 750 training observations. Beyond this point, the metric stays the same. This means adding additional training data won't noticeably improve the model performance. So, for this dataset, it may be better to consider\n",
    "   - adding more features to the data, as this is very likely to increase complexity of the model and improve performance\n",
    "   - switching to a non-linear regression model that can build a more complicated regressor from the existing data\n",
    "     - from the learning curves for the `RandomForestRegressor`, we see that this is precisely the case\n",
    "       - the training and validation scores are nearly doubled\n",
    "       - these more complicated models generally require more data, and we can see this from the validation score which has not reached a plateau.i.e. it could benefit from more validation data.\n",
    "2. The main indicator of a bias problem is a high validation error. The validation metric here, for the `RidgeCV` model, reaches a plateau at approx. \\$5,000 which is right at our desired model performance indicated earlier.\n",
    "   - For each listing, the model is off by this amount on average. So, the best linear model found here has a problem of moderate bias.\n",
    "   - since the training error is also right at our desired performance, it means that the training data is also not fit well enough by the best linear model. This means the best model found here has moderate bias with respect to the training data.\n",
    "3. A consistently narrow gap exists between the training and validation learning urves. This indicates low variance. The narrower the gap, the lower the variance for the following reasons\n",
    "   - if the variance is high, then the model fits training data too well\n",
    "     - when training data is fitted too well, the model will have trouble generalizing on data that hasn’t seen in training. When such an overfitting model is tested on its training set, and then on a validation set, the training error will be low and the validation error will generally be high. As we change training set sizes, this pattern continues, and the differences between training and validation errors will determine that gap between the two learning curves\n",
    "     - the larger the difference between the two errors, the larger the variance\n",
    "     - in this case, the gap is very narrow, indicating that the variance is low\n",
    "   - high training RMSE scores also indicate low variance\n",
    "     - if the variance is low, then the model will generate similar but over-simplified models as the length of the training observations is increased. Because the models are overly simplified, they do not fit the training data well enough, producing should a high training error.\n",
    "4. Overall, the best linear model here suffers from a moderate bias and low variance problem. Adding more training observations is unlikely to improve this much. Two approaches to remedy this are\n",
    "   - remove or decrease regularization\n",
    "     - this will allow the model to better fit the training data and increase the variance as well as decrease the bias\n",
    "       - in order to do this while preventing model coefficients from blowing up, as seen earlier for `LinearRegression`, some regularization is likely necessary for the current dataset\n",
    "   - add more features\n",
    "     - this requrires scraping more data or using a new set of features than the ones chosen at the start of this analysis (`v1`)\n",
    "   - try a non-linear model\n",
    "     - as seen from the learning curves for the `RandomForestRegressor` model, this approach improves the model's performance\n",
    "     - the learning curves for this model suggest a low bias and high variance performance\n",
    "       - the low training error backs up this claim of high variance\n",
    "       - the validation curve does not plateau at the maximum tested training set size\n",
    "         - it can still decrease and converge towards the training curve\n",
    "       - the large gap between training and validation curves indicates overfitting and could be remedied using\n",
    "         - added training and validation data, since both scores are trending towards eachother, and will converge with more data\n",
    "         - reduced number of features, perhaps using feature selection\n",
    "         - hyperparameter optimization\n",
    "         - increased regularization, by increasing the `max_leaf_nodes` hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion-and-future-work\"></a>\n",
    "\n",
    "### 10. [Conclusion and Future work](#conclusion-and-future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "\n",
    "#### 10.1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project was to predict listed vehicle prices to within \\$5,000. After data pre-processing and model development, the best model was trained on approx. 2,200 observations and made approx. 600 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model here scores an RMSE, on the predictions, of approx. \\$4,900 (`R^2` = 0.61) which meets the goal for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"future-work\"></a>\n",
    "\n",
    "#### 10.1. [Future work](#future-work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limitation of this model is due to its moderate bias and low variance. The following are some areas where the current work could be expanded to explore improvement in model performance\n",
    "\n",
    "1. As found from analysis of the learning curves for the best linear model, extract more features from the scraped data such as\n",
    "   - additional attributes of the listing in the text of the description section such as (but not limited to)\n",
    "     - horsepower\n",
    "     - whether the car is a luxury brand or not\n",
    "     - number of doors\n",
    "     - financing options\n",
    "   - explore non-linearities in newly extracted features\n",
    "2. Numerical features were standardized by rescaling the distribution of their values so that the mean of observed values is 0 and the standard deviation is 1. A useful comparison could involve exploring the effect of normalized by rescaling the data so that all values are within the range of 0 and 1.\n",
    "3. Considering that the five worst predictions of the model were in Washington state, more stringent removal of outliers should be explored on a per-feature basis and in the context of their relation to the listing prices. This is also warranted to explore the relationship between train_validation score and test set set score.i.e. to get a better estimate of the model's generalization error.\n",
    "4. Feature selection techniques could be explored, using cross-validation, to decrease the number of input features.\n",
    "5. Categorical features could be aggregated so that infrequently occurring features are gathered into a new category named `Other`. This could help reduce the cardinality of some of the categorical features. The disadvantage is that some of the predictive power of the feature could be lost since previously useful independent categories are combined into a possibly less useful aggregated one. The usefulness of this approach should be explored via cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
